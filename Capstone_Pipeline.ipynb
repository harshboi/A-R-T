{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import twint\n",
    "import json\n",
    "from bert_serving.client import BertClient\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import pdb\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Main For calling all other functions\n",
    "#Make sure server is running before you start this\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nest_asyncio.apply()\n",
    "#Step 1 - Pulling Tweets\n",
    "tags = getTags()\n",
    "tags = processTags(tags)\n",
    "scrapeTweets(tags)\n",
    "data = processData()\n",
    "#Step 2 -Collect Tweet Embeddings and Step 3- Pass Embeddings through svm_model.p to get Labels\n",
    "data = cleanData(data)\n",
    "# getEncodings(newTweets) Called from cleanData function!\n",
    "classifyTweets(\"svm_model.p\",\"new_text.npy\",\"encoded_file.npy\",\"classifiedTweets.txt\",data)\n",
    "#Step 4 - Keep Only Relevant Tweets and Pass through NLTK\n",
    "data = removeIrrelevant(data)\n",
    "data = applyNLTK(data)\n",
    "data = applySpacy(data)\n",
    "#Step 5 -Send to GraphDB\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all security tags that we will be searching twitter with\n",
    "def getTags():\n",
    "    #Reads tags from file  and adds each tag to an array\n",
    "    file = open(\"./security_tags.txt\",\"r\")\n",
    "    arr = []\n",
    "    line1 = file.readlines()\n",
    "    for x in line1:\n",
    "        arr.append(x)\n",
    "    file.close()\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the tags so they are ready to be used with Twint\n",
    "def processTags(arr):\n",
    "    #Removes the hashtag from every tag\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = arr[i][1:]\n",
    "    file = open(\"./test1.json\",\"w\")\n",
    "    file.close()\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use twint to collect tweets\n",
    "def scrapeTweets(arr):\n",
    "    #Iterates through all security terms and searches twitter, writes all the data to a file called test1.json\n",
    "    for i in range(len(arr)):\n",
    "        d = twint.Config()\n",
    "        d.Search = arr[i]\n",
    "        d.Limit = 20\n",
    "        # d.Store_csv = True\n",
    "        d.Store_object = True\n",
    "        d.Store_json = True\n",
    "        d.Output = \"./test1.json\"\n",
    "        x = twint.run.Search(d)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process data collected from twint so it is ready to be encoded by bert\n",
    "def processData():\n",
    "    f= open(\"./test1.json\",\"r\",errors='ignore')\n",
    "    line1 = f.readlines()\n",
    "    arr = []\n",
    "    for x in line1:\n",
    "        arr.append(x)\n",
    "    f.close() \n",
    "    arr.insert(0,\"[\")\n",
    "    arr.append(']')\n",
    "    for i in range(1,len(arr)-2):\n",
    "        arr[i] = arr[i][:-1] + \",\" + arr[i][-1:]\n",
    "    file = open(\"../test.json\",\"w\")\n",
    "    for i in range(len(arr)):\n",
    "        file.write(arr[i])\n",
    "    file.close()\n",
    "    f= open(\"../test.json\",\"r\",errors='ignore')\n",
    "    line1 = f.readlines()\n",
    "    data = []\n",
    "    for x in line1:\n",
    "        data.append(x)\n",
    "    f.close()\n",
    "    for i in range(len(data)):\n",
    "        data[i] = (''.join([i if ord(i) < 128 else ' ' for i in str(data[i])]))\n",
    "    file = open(\"./data.json\",\"w\")\n",
    "    for i in range(len(data)):\n",
    "        file.write(data[i])\n",
    "    file.close()\n",
    "    with open(\"./data.json\", \"r\",errors='ignore') as read_file:\n",
    "        data = json.load(read_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEncodings(data):\n",
    "    #Remove lenght requirements as some of the tweets are longer and it will throw an error\n",
    "    #Gets encodings from BaaS and creates a npy file with encodings called encoded_file.npy\n",
    "    bc = BertClient(check_length = False)\n",
    "    text_file_name = \"new_text.npy\"\n",
    "    encoding_filename = \"encoded_file.npy\"\n",
    "    np.save(text_file_name,data)\n",
    "    print(\"Finished saving labels, beginning encoding process, this might take a couple minutes!\\nWe will notify on completion\")\n",
    "    encodings = bc.encode(data)\n",
    "    np.save(encoding_filename,encodings)\n",
    "    print(\"Completed encoding and saving!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data for encoding\n",
    "def cleanData(data):\n",
    "    #parse json data into two arrays containing the tweet texts and their correponding relevancy label\n",
    "    #track numbers of relevant and irrelevant tweets \n",
    "    relnum =0\n",
    "    irrnum = 0\n",
    "    text = []\n",
    "    labels = []\n",
    "    newData = []\n",
    "    seen_urls = set()\n",
    "    duplicate_counter = 0\n",
    "    \n",
    "    for x in data:\n",
    "#          Dont need to worry about duplicates for streaming\n",
    "#         if re.sub(r\"http\\S+\", \"\", x['tweet']) in seen_urls:\n",
    "#             duplicate_counter +=1\n",
    "#             continue\n",
    "#         else:\n",
    "#             seen_urls.add(re.sub(r\"http\\S+\", \"\", x['tweet']))\n",
    "\n",
    "        text.append(re.sub(r\"http\\S+\", \"\", x['tweet']))\n",
    "    \n",
    "\n",
    "    for i in data:\n",
    "        if(re.match('[a-zA-Z]',i['tweet'])):\n",
    "            newData.append(i)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    newtext = [str for str in text if re.match('[a-zA-Z]', str)]\n",
    "    data =newData\n",
    "    getEncodings(newtext)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyTweets(model_file,text_file,encoding_file,output_file,data):\n",
    "    #Uses model stored in svm_model.p to get predictions and we add these predictions to our data array\n",
    "    clf = pickle.load(open(model_file,'rb'))\n",
    "    text = np.load(text_file)\n",
    "    x = np.load(encoding_file)\n",
    "    predictions = clf.predict(x)\n",
    "    print_arr = ['Irrelevant','Relevant']\n",
    "    f = open(output_file, 'w')\n",
    "    f.close()\n",
    "    counter = 0\n",
    "    for i,t in enumerate(text):\n",
    "        f = open(output_file, 'a')\n",
    "        f.write(\"Tweet: \"+t+'\\n')\n",
    "        f.write(\"Prediction: \"+str(print_arr[predictions[i]])+'\\n\\n')\n",
    "        f.close()\n",
    "        data[counter]['Relevance'] = str(print_arr[predictions[i]])\n",
    "        counter+=1\n",
    "        print(\"Tweet: \"+t)\n",
    "        print(\"Prediction: \"+str(print_arr[predictions[i]])+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeIrrelevant(data):\n",
    "    #Cleans up our data by only keeping relevant tweets\n",
    "    tempData = []\n",
    "    for x in data:\n",
    "        if x['Relevance'] == \"Relevant\":\n",
    "            tempData.append(x)\n",
    "        else:\n",
    "            continue\n",
    "    return tempData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyNLTK(data):\n",
    "    #Runs all the tweets through nltk and stores all the nouns for each tweet in the data object\n",
    "    for x in data:\n",
    "        nouns = []\n",
    "        tokens = nltk.word_tokenize(x['tweet'])\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        for i in range(len(tagged)):\n",
    "            if (tagged[i][1] == 'NNP'):\n",
    "                nouns.append(tagged[i][0])\n",
    "        x['nltk'] = nouns\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySpacy(data):\n",
    "    #Runs all the tweets through Spacy and stores the words extracted in the data object\n",
    "    for x in data:\n",
    "        doc = nlp(x['tweet'])\n",
    "        x['spacy'] = [ent for ent in doc.ents]\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
