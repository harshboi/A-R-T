{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Regular_Twitter_Corpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-52aff9ddb3a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../Regular_Twitter_Corpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./concatenated_data.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Regular_Twitter_Corpus'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import json\n",
    "directory = os.getcwd()\n",
    "os.chdir(\"../../Regular_Twitter_Corpus\")\n",
    "output_file = \"./concatenated_data.json\"\n",
    "text = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/psingh4/Desktop/Harsh/Regular_Twitter_Corpus'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.json\n",
      ".DS_Store\n",
      "37.json\n",
      "30.json\n",
      "31.json\n",
      "32.json\n",
      "29.json\n",
      "34.json\n",
      "38.json\n",
      "39.json\n",
      "35.json\n"
     ]
    }
   ],
   "source": [
    "total_data = []\n",
    "\n",
    "# If Json does not have [] at the beginning and end and commas at the end of each row. Give full address\n",
    "def create_json_from_json_row_dump ( directory, filename )\n",
    "    os.chdir(directory)\n",
    "    \n",
    "    f = open(\"./\"+filename,\"r\", errors='replace')\n",
    "    line1 = f.readlines()\n",
    "    arr = []\n",
    "\n",
    "    for x in line1:\n",
    "        arr.append(x)\n",
    "    f.close()  \n",
    "    \n",
    "    arr.insert(0,\"[\")\n",
    "    arr.append(']')\n",
    "    for i in range(1,len(arr)-2):\n",
    "        arr[i] = arr[i][:-1] + \",\" + arr[i][-1:]\n",
    "    \n",
    "    file = open(filename,\"w\")\n",
    "    for i in range(len(arr)):\n",
    "        file.write(arr[i])\n",
    "    file.close()\n",
    "\n",
    "# Only returns the body of a tweet scraped using the twitter API\n",
    "def only_return_tweet ( directory, filename ):\n",
    "    os.chdir(directory)\n",
    "    if (filename == \".DS_Store\"):\n",
    "        continue\n",
    "    with open(filename,'rb') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    for x in data:\n",
    "        if 'full_text' in x.keys():\n",
    "            text.append(x['full_text'])\n",
    "            total_data.append({\"text\": x['full_text'], })\n",
    "        elif 'text' in x.keys():\n",
    "            text.append(x['text'])\n",
    "            total_data.append({\"text\": x['text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files ( directory, total_data, ignore_list ):\n",
    "    for filename in os.listdir(directory):\n",
    "        data = {}\n",
    "        print(filename)\n",
    "        if (filename == \"ignore.txt\"):\n",
    "            continue\n",
    "        with open(filename,'rb') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        for x in data['statuses']:\n",
    "            if x['lang'] != \"en\":\n",
    "                continue\n",
    "            if  x['id'] in ignore_list:\n",
    "                continue\n",
    "            if 'full_text' in x.keys():\n",
    "                text.append(x['full_text'])\n",
    "                total_data.append({\"text\": x['full_text'], \"relevant\": 1})\n",
    "            elif 'text' in x.keys():\n",
    "                text.append(x['text'])\n",
    "                total_data.append({\"text\": x['text'], \"relevant\": 1})\n",
    "\n",
    "        print(len(text))\n",
    "# np.save(output_file,text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd()\n",
    "total_data = []\n",
    "\n",
    "\n",
    "with open(\"ignore.txt\",'r') as file:\n",
    "    x = file.readlines()\n",
    "ignore_list = []\n",
    "for i in range(len(x)):\n",
    "    ignore_list.append(int(x[i]))\n",
    "ignore_list\n",
    "\n",
    "\n",
    "concat_files(directory, total_data, ignore_list)\n",
    "\n",
    "os.chdir(\"../security\")\n",
    "directory = os.getcwd()\n",
    "with open(\"ignore.txt\",'r') as file:\n",
    "    x = file.readlines()\n",
    "ignore_list = []\n",
    "for i in range(len(x)):\n",
    "    ignore_list.append(int(x[i]))\n",
    "ignore_list\n",
    "\n",
    "concat_files(directory, total_data, ignore_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from googletrans import Translator\n",
    "\n",
    "def is_english(sentence):\n",
    "    if translator.detect(sentence).lang == 'en':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "with open(\"../classified_tweets.json\", \"r\",errors='ignore') as read_file:\n",
    "    ct_data = json.load(read_file)\n",
    "\n",
    "translator = Translator(service_urls=['translate.google.com'])\n",
    "i=0\n",
    "skip = [130]\n",
    "while i <= len(ct_data):\n",
    "    print(i)\n",
    "    if (i in skip): \n",
    "        i += 1\n",
    "        continue\n",
    "#     print(ct_data[i]['tweet'])\n",
    "    if is_english(ct_data[i]['tweet']) == False:\n",
    "        print(ct_data[i]['tweet'])\n",
    "        ct_data.pop(i)\n",
    "    else: i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "\n",
    "with open(\"all_train.json\",'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for dic in data:\n",
    "    total_data.append({'text': dic['tweet'], 'relevant': dic['relevant']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "# Is for final Json document\n",
    "#########################################################################################################\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# with open(\"all_train.json\",'rb') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "\n",
    "for dic in ct_data:\n",
    "    total_data.append({'text': dic['tweet'], 'relevant': dic['relevant']})\n",
    "    \n",
    "# Dumps all the data into one final json document\n",
    "os.chdir(\"../\")\n",
    "json_object = json.dumps(total_data, indent = 4) \n",
    "with open(\"complete_data1.json\", \"w\") as outfile: \n",
    "    outfile.write(json_object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "# Explore Input data\n",
    "# NOTE CELL WILL NOT WORK TILL WORKING DIRECTORY SET TO GENERAL\n",
    "#########################################################################################################\n",
    "\n",
    "data = {}\n",
    "with open(\"actors.json\",'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "data.keys()\n",
    "len(data['search_metadata'])\n",
    "data['statuses'][1]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "\n",
    "with open(\"all_train.json\",'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for dic in data:\n",
    "    total_data.append({'text': dic['tweet'], 'relevant': dic['relevant']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"classified_tweets.json\",'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for dic in data:\n",
    "    total_data.append({'text': dic['tweet'], 'relevant': dic['relevant']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_tweets_classified.json\",'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for dic in data:\n",
    "    total_data.append({'text': dic['tweet'], 'relevant': dic['relevant']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
