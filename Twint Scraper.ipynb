{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "file = open(\"../security_tags.txt\",\"r\")\n",
    "arr = []\n",
    "line1 = file.readlines()\n",
    "for x in line1:\n",
    "    arr.append(x)\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(arr)):\n",
    "    arr[i] = arr[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./test1.json\",\"w\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear file\n",
    "#Twint Search\n",
    "for i in range(len(arr)):\n",
    "    d = twint.Config()\n",
    "    d.Search = arr[i]\n",
    "    d.Limit = 50\n",
    "    # d.Store_csv = True\n",
    "    d.Store_object = True\n",
    "    d.Store_json = True\n",
    "    d.Output = \"./test1.json\"\n",
    "    # d.Database = \"tweets.db\"\n",
    "\n",
    "\n",
    "    x = twint.run.Search(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238779571249401856 2020-03-14 03:50:39 Pacific Daylight Time <mizdra> __proto__ Ê±öÊüì„Åã NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) ¬∑ CVE-2020-7598 ¬∑ GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7¬†‚Ä¶\n",
      "1238092404185587718 2020-03-12 06:20:06 Pacific Daylight Time <bins3c> @malwrhunterteam my dog just sniffed some around and found a CVE-2020-0796 scanner on Github.  https://github.com/dickens88/cve-2020-0796-scanner/¬†‚Ä¶ üòÄ#CoronaBlue #CyberSecurity\n",
      "1237850601024671748 2020-03-11 14:19:16 Pacific Daylight Time <fabio_viggiani> CVE-2020-0796, SMBGhost, pre-auth RCE in SMBv3. It has all the potential to become the new EternalBlue. And there is no patch for it now. I am seeing some Github projects popping up, I'm sure we'll soon see a weaponizable PoC. Fasten you seatbelts folks... #smbghost #cve20200796\n",
      "1237752777125826560 2020-03-11 07:50:33 Pacific Daylight Time <darksh3llGR> CVE-2020-0796. Scan HOST/CIDR with nmap script smb-protocols.nse and grep SMB version 3.11. ¬∑ GitHub  https://gist.github.com/nikallass/40f3215e6294e94cde78ca60dbe07394¬†‚Ä¶\n",
      "1233655666805334016 2020-02-28 23:30:05 Pacific Daylight Time <motakasoft> GitHub Trending Archive, 27 Feb 2020, C++. JustasMasiulis/inline_syscall, afang5472/CVE-2020-0753-and-CVE-2020-0754, 0vercl0k/kdmp-parser, idebtor/nowic, harvestlamb/Cpp_houjie, kervin521/navicat-keygen, hao14293/2021-Postgraduate-408  https://github.motakasoft.com/trending/?d=2020-02-27&l=c¬†‚Ä¶++\n",
      "1233622987481452544 2020-02-28 21:20:14 Pacific Daylight Time <securisec> \"RT RT raycp2: it took me two days to finish the analysis of cve-2020-6418, amazing vuln, learned a lot from this bug. thanks to _clem1   here is the github repo:  https://github.com/ray-cp/browser_pwn/tree/master/cve-2020-6418¬†‚Ä¶ and here is the writeup:  https://ray-cp.github.io/archivers/browser-pwn-cve-2020-6418%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90¬†‚Ä¶\"\n",
      "1233426023968104451 2020-02-28 08:17:34 Pacific Daylight Time <raycp2> it took me two days to finish the analysis of cve-2020-6418, amazing vuln, learned a lot from this bug. thanks to @_clem1   here is the github repo:  https://github.com/ray-cp/browser_pwn/tree/master/cve-2020-6418¬†‚Ä¶ and here is the writeup:  https://ray-cp.github.io/archivers/browser-pwn-cve-2020-6418%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90¬†‚Ä¶\n",
      "1233293277131993093 2020-02-27 23:30:05 Pacific Daylight Time <motakasoft> GitHub Trending Archive, 26 Feb 2020, C++. JustasMasiulis/inline_syscall, afang5472/CVE-2020-0753-and-CVE-2020-0754, idebtor/nowic, harvestlamb/Cpp_houjie, hao14293/2021-Postgraduate-408, danielkrupinski/Osiris, tindy2013/subconverter  https://github.motakasoft.com/trending/?d=2020-02-26&l=c¬†‚Ä¶++\n",
      "1233071754630877185 2020-02-27 08:49:50 Pacific Daylight Time <HackingLZ> A few point and shoot Exchange CVE-2020-0688 exploits on GitHub now  https://github.com/search?q=CVE-2020-0688¬†‚Ä¶\n",
      "1231941112560050179 2020-02-24 05:57:04 Pacific Daylight Time <0xbfho> boh starred afang5472/CVE-2020-0753-and-CVE-2020-0754 on Github  https://ift.tt/2PjBWsk¬†\n",
      "1227602278443601920 2020-02-12 06:36:05 Pacific Daylight Time <mkolsek> Nice! Do I understand correctly that for CVE-2020-0738 to be exploited, one has to have some application installed that is using Microsoft Media Foundation, and have a malicious file processed by that app? (I see you used a sample app from MS's GitHub.)\n",
      "1225777527441981441 2020-02-07 05:45:11 Pacific Daylight Time <CVEnew> CVE-2020-8788 Synaptive Medical ClearCanvas ImageServer 3.0 Alpha allows XSS (and HTML injection) via the Default.aspx UserName parameter. NOTE: the issues/227 reference does not imply that the affected product can be downloaded from GitHub. It was simp...  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8788¬†‚Ä¶\n",
      "1223332930577215488 2020-01-31 11:51:13 Pacific Daylight Time <VulmonFeeds> CVE-2020-5234  MessagePack for C# and Unity before version 1.9.3 and 2.1.80 has a vulnerability where untrusted data can lead to DoS attack due to hash collisions and stack overflow. Review the linked GitHub Securit...   http://vulmon.com/vulnerabilitydetails?qid=CVE-2020-5234¬†‚Ä¶\n",
      "1223316335322640384 2020-01-31 10:45:17 Pacific Daylight Time <CVEnew> CVE-2020-5234 MessagePack for C# and Unity before version 1.9.3 and 2.1.80 has a vulnerability where untrusted data can lead to DoS attack due to hash collisions and stack overflow. Review the linked GitHub Security Advisory for more information and rem...  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5234¬†‚Ä¶\n",
      "1220006475252166656 2020-01-22 07:33:04 Pacific Daylight Time <DoxElectronics> An alert was issued for @Microsoft Cryptographic Library CRYPT32.DLL Jan. 16, 2020. Researchers from Kudelski Security and Ollypwn have published a proof-of-concept for CVE-2020-0601 to GitHub. The vulnerability could lead to a breach.  Learn more at  https://www.doxnet.com/2020/01/microsoft-cryptographic-library-crypt32-dll-vulnerability-proof-of-concept-released/¬†‚Ä¶. pic.twitter.com/MGArebkMFf\n",
      "1217747869957935104 2020-01-16 01:58:11 Pacific Daylight Time <ChadBrubaker5> And a MiTM test for cve-2020-0601 is added to nogotofail, in case you wanted a black box network testing tool it's over on github\n"
     ]
    }
   ],
   "source": [
    "# clear file\n",
    "#Twint Search\n",
    "d = twint.Config()\n",
    "d.Search = \"CVE-2020-* and github\"\n",
    "# d.Search = \"ransomware AND seminar\"\n",
    "d.Limit = 50\n",
    "# d.Store_csv = True\n",
    "d.Store_object = True\n",
    "d.Store_json = True\n",
    "d.Output = \"./test1.json\"\n",
    "# d.Database = \"tweets.db\"\n",
    "\n",
    "x = twint.run.Search(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search terms\n",
    "# 0-day AND exploit\n",
    "# ‚Äú0-day‚Äù, ‚ÄúCVE-‚Äú, ‚ÄúCVE-2018-*‚Äù, ‚ÄúCVE-2020-*‚Äù\n",
    "# usernames:\n",
    "# kibbsy\n",
    "# it_securitynews\n",
    "# msftsecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = twint.run.Search(d)\n",
    "# twint -u noneprivacy --csv --output \"/Users/psingh4/harsh/test4.json\" --lang en --translate --translate-dest it --limit 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twint.run.Search(d)\n",
    "# twint.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"./test1.json\",\"r\",errors='ignore')\n",
    "line1 = f.readlines()\n",
    "arr = []\n",
    "for x in line1:\n",
    "    arr.append(x)\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.insert(0,\"[\")\n",
    "arr.append(']')\n",
    "for i in range(1,len(arr)-2):\n",
    "    arr[i] = arr[i][:-1] + \",\" + arr[i][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../test.json\",\"w\")\n",
    "for i in range(len(arr)):\n",
    "    file.write(arr[i])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"../test.json\",\"r\",errors='ignore')\n",
    "line1 = f.readlines()\n",
    "data = []\n",
    "for x in line1:\n",
    "    data.append(x)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing non ASCII characters\n",
    "for i in range(len(data)):\n",
    "    data[i] = (''.join([i if ord(i) < 128 else ' ' for i in str(data[i])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data.json\",\"w\")\n",
    "for i in range(len(data)):\n",
    "    file.write(data[i])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./data.json\", \"r\",errors='ignore') as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = mysql.connector.connect(user='admin', password='Private2712!',\n",
    "                              host='database-1.cok63qqiofsd.us-east-1.rds.amazonaws.com',\n",
    "                              database='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"INSERT INTO train_data (created_at, conversation_id, id, date, time, timezone, user_id, username, name, place, tweet, mentions, urls, replies_count, \n",
    "        retweets_count, likes_count, hashtags, cashtags, link, retweet, video, near, geo, source, user_rt_id, user_rt, retweet_id,\n",
    "        retweet_date, translate, trans_src, trans_dest, photos, reply_to) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, \n",
    "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "\n",
    "val = []\n",
    "for i in range(1, len(data)):\n",
    "    val.append((data[i]['created_at'], data[i]['conversation_id'], str(data[i]['id']), data[i]['date'], data[i]['time'], data[i]['timezone'], data[i]['user_id'], data[i]['username'],\n",
    "      data[i]['name'], data[i]['place'], data[i]['tweet'], str(data[i]['mentions']), str(data[i]['urls']), data[i]['replies_count'],\n",
    "      data[i]['retweets_count'], data[i]['likes_count'], str(data[i]['hashtags']), str(data[i]['cashtags']), data[i]['link'], data[i]['retweet'],\n",
    "      data[i]['video'], data[i]['near'], data[i]['geo'], data[i]['source'], data[i]['user_rt_id'], data[i]['user_rt'],\n",
    "      data[i]['retweet_id'], data[i]['retweet_date'], data[i]['translate'], data[i]['trans_src'], data[i]['trans_dest'],str(data[i]['photos']), \n",
    "       str(data[i]['reply_to'])))\n",
    "    \n",
    "mycursor.executemany(sql, val)\n",
    "\n",
    "mydb.commit()\n",
    "\n",
    "print(mycursor.rowcount, \"record inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "for i in range(len(data)):\n",
    "    val.append((data[i]['created_at'], data[i]['conversation_id'], data[i]['date'], data[i]['time'], data[i]['timezone'], data[i]['user_id'], data[i]['username'],\n",
    "      data[i]['name'], data[i]['place'], data[i]['tweet'], str(data[i]['mentions']), str(data[i]['urls']), data[i]['replies_count'],\n",
    "      data[i]['retweets_count'], data[i]['likes_count'], str(data[i]['hashtags']), str(data[i]['cashtags']), data[i]['link'], data[i]['retweet'],\n",
    "      data[i]['video'], data[i]['near'], data[i]['geo'], data[i]['source'], data[i]['user_rt_id'], data[i]['user_rt'],\n",
    "      data[i]['retweet_id'], data[i]['retweet_date'], data[i]['translate'], data[i]['trans_src'], data[i]['trans_dest'],0,str(data[i]['photos']), \n",
    "       str(data[i]['reply_to'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydb.commit()\n",
    "# !pip install tensorflow\n",
    "# print(mycursor.rowcount, \"record inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert-serving-server\n",
    "# !pip install bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"NSA exploit EternalRomance with CVE-2019-897 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "# sentence = \"NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) ¬∑ CVE-2020-7598 ¬∑ GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NSA', 'NNP'),\n",
       " ('exploit', 'VBZ'),\n",
       " ('EternalRomance', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('CVE-2019-897', 'NNP'),\n",
       " (\"'formed\", 'POS'),\n",
       " ('basis', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('alleged', 'VBN'),\n",
       " ('Chinese', 'JJ'),\n",
       " ('tool', 'NN'),\n",
       " (\"'\", 'POS'),\n",
       " ('-', ':'),\n",
       " ('iTWire', 'NN'),\n",
       " ('Researchers', 'NNP'),\n",
       " ('Mark', 'NNP'),\n",
       " ('Lechtik', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Nadav', 'NNP'),\n",
       " ('Grossman', 'NNP'),\n",
       " ('said', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('first', 'JJ'),\n",
       " ('offered', 'VBN'),\n",
       " ('NSA', 'NNP'),\n",
       " ('exploits', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('hacking', 'VBG'),\n",
       " ('group', 'NN'),\n",
       " (',', ','),\n",
       " ('known', 'VBN'),\n",
       " ('variously', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('APT3', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Gothic', 'NNP'),\n",
       " ('Panda', 'NNP'),\n",
       " (',', ','),\n",
       " ('had', 'VBD'),\n",
       " ('based', 'VBN'),\n",
       " ('its', 'PRP$'),\n",
       " ('exploitation', 'NN'),\n",
       " ('tool', 'NN'),\n",
       " (',', ','),\n",
       " ('https', 'NN'),\n",
       " (':', ':'),\n",
       " ('//ift.tt/2HRG9js', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) ¬∑ CVE-2020-7598 ¬∑ GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7\n",
      "[NowBrowsing, CVE-2020-7598]\n"
     ]
    }
   ],
   "source": [
    "# sentence = \"NSA exploit EternalRomance with CVE-2019-897 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "sentence = \"NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) ¬∑ CVE-2020-7598 ¬∑ GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bert-serving-server bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bert-serving-start -model_dir /tmp/english_L-12_H-768_A-12/ -num_worker=4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bert_serving.client import BertClient\n",
    "# bc = BertClient()\n",
    "# bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first neural network with keras tutorial\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# load the dataset\n",
    "data = np.load('./tweet_encodings.npy')\n",
    "label = np.load('./maybeincludedlabels.npy')\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(400, input_dim=768, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "# evaluate the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data, label, epochs=10, batch_size=1, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "len(data), len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=1).fit(data[:-900], label[:-900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(data[-900:], label[-900:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(data[-800:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = label[-800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(y_true,y_pred)\n",
    "print(\"Accuracy score: {}\".format(accuracy_score))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true,y_pred,pos_label = 1)\n",
    "auc_score = metrics.auc(fpr,tpr)\n",
    "print(\"AUC: {}\".format(auc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def print_metrics(y_true,y_pred):\n",
    "    accuracy_score = metrics.accuracy_score(y_true,y_pred)\n",
    "    print(\"Accuracy score: {}\".format(accuracy_score))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true,y_pred,pos_label = 1)\n",
    "    auc_score = metrics.auc(fpr,tpr)\n",
    "    print(\"AUC: {}\".format(auc_score))\n",
    "    return accuracy_score,auc_score\n",
    "\n",
    "def logistic_regression(x_train,x_test,y_train,y_test):\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(x_train,y_train)\n",
    "    predictions = logisticRegr.predict(x_test)\n",
    "    print_metrics(y_test,predictions)\n",
    "\n",
    "\n",
    "def support_vector(x_train,x_test,y_train,y_test):\n",
    "    model = svm.SVC()\n",
    "    model.fit(x_train,y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    print_metrics(y_test,predictions)\n",
    "\n",
    "def torch_label_creator(label):\n",
    "    if label.item() == 1:\n",
    "        return torch.tensor([1,0])\n",
    "    else:\n",
    "        return torch.tensor([0,1])\n",
    "\n",
    "    #plot test accuracies and auc over epochs    \n",
    "    plt.plot(test_accuracies)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Test Accuracy vs Epoch')\n",
    "    plt.show()\n",
    "    plt.plot(test_auc)\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Test AUC vs Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = np.load('tweet_encodings.npy')\n",
    "labels = np.load('maybeincludedlabels.npy')\n",
    "x_train,x_test,y_train,y_test = train_test_split(encodings,labels, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged[0][1] == 'NNP'\n",
    "noun = []\n",
    "for i in range(len(data)):\n",
    "    sentence = data[i]['tweet']\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    sen_noun = []\n",
    "    for j in range(len(tagged)):\n",
    "        if (tagged[j][1] == 'NNP'):\n",
    "            sen_noun.append(tagged[j][0])\n",
    "    noun.append(sen_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost\",auth=(\"test_user\",\"password\"))\n",
    "#Parse data\n",
    "counter = 0\n",
    "with open(\"./data.json\",'r') as file:\n",
    "    #Need to make a session where you will run all your cypher queries\n",
    "    with driver.session() as session:\n",
    "        tx = session.begin_transaction()\n",
    "        noun_count = 0\n",
    "        count = 0\n",
    "        for line in file.readlines(): #can limit lines with [:100] after ()\n",
    "            if (line == \"[\\n\" or line == \"]\"):\n",
    "                continue\n",
    "            item = json.loads(line[:-2])\n",
    "            sen_noun = noun[noun_count]\n",
    "            # print(item)\n",
    "            '''WITH {tweet} AS Tweet\n",
    "                Merge (a:Tweet{id:$value.id,date:value.date,train_id:value.train_id})\n",
    "                Merge (d:Date{date:value.date})\n",
    "                Merge (a)-[:SAME_DATE]->(d)'''\n",
    "            '''CREATE (a:Tweet{id:$value.id,date:$value.date,train_id:$value.train_id,nouns=sen_noun})'''\n",
    "            '''start n=node(*) return n'''\n",
    "            tx.run('''CREATE (a:Tweet{id:$value.id,date:$value.date,train_id:$value.train_id,nouns:$sen_noun})''',\n",
    "                   parameters={'tweet': item}, value=item, sen_noun=sen_noun) # add parameters here\n",
    "            #Batch processing to run 1000 tweets as a time as these commits are quite time intensive\n",
    "            count += 1\n",
    "            noun_count += 1\n",
    "#             tx.commit()\n",
    "#             break   # UNCOMMENT IF PUSHING MULTIPLE ROWS/NODES OF DATA \n",
    "            if count > 1000:\n",
    "                tx.commit()\n",
    "                tx = session.begin_transaction()\n",
    "                count = 0\n",
    "        tx.commit()\n",
    "# import json\n",
    "# with open(\"./data.json\", \"r\") as read_file:\n",
    "#     for line in read_file.readlines():\n",
    "#         print(line)\n",
    "#         if (line == \"[\\n\" or line == \"]\"):\n",
    "#             continue\n",
    "#         item = json.loads(line[:-2])\n",
    "#         print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cypher code for graph db\n",
    "# Match (a:Tweet) where 'Malware' in a.nouns return a\n",
    "# start n=node(*) return n\n",
    "# Match (a:Tweet), (b:Tweet) where 'Malware' in a.nouns and 'Malware' in b.nouns create (a)-[r:Malware]->(b) return r\n",
    "\n",
    "#     MATCH (n:Tweet)-[m:Malware]-(b:Tweet)\n",
    "#     WHERE n.id = 1223562341540859904\n",
    "#     REturn b\n",
    "#     order by b.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentence = \"NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NSA',\n",
       " 'EternalRomance',\n",
       " 'Researchers',\n",
       " 'Mark',\n",
       " 'Lechtik',\n",
       " 'Nadav',\n",
       " 'Grossman',\n",
       " 'NSA',\n",
       " 'APT3',\n",
       " 'Gothic',\n",
       " 'Panda']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_list = []\n",
    "for i in range(len(tagged)):\n",
    "    if (tagged[i][1] == 'NNP'):\n",
    "        noun_list.append(tagged[i][0])\n",
    "noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost\",auth=(\"test_user\",\"password\"))\n",
    "\n",
    "with driver.session() as session:\n",
    "    tx = session.begin_transaction()\n",
    "    count = 0\n",
    "    for noun in noun_list:\n",
    "        tx.run('''Match (a:Tweet), (b:Tweet) where $noun in a.nouns and $noun in b.nouns \n",
    "        merge (a)-[r:'''+ noun.upper() + ''']->(b)''',\n",
    "               noun=noun, cap_noun=noun.upper()) # add parameters here\n",
    "        #Batch processing to run 1000 tweets as a time as these commits are quite time intensive\n",
    "    count += 1\n",
    "    if count > 1000:\n",
    "        tx.commit()\n",
    "        tx = session.begin_transaction()\n",
    "        count = 0\n",
    "    tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\"bolt://localhost\",auth=(\"test_user\",\"password\"))\n",
    "x = None\n",
    "with driver.session() as session:\n",
    "    tx = session.begin_transaction()\n",
    "    result = tx.run(\"Match (a:Tweet),(b:Tweet) where 'Malware' in a.nouns and 'Malware' in  b.nouns return a,b\")\n",
    "    x = result.records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.single()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "for i in x:\n",
    "    xx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = hash(xx)\n",
    "xx[2].value(key=1)\n",
    "# xx[2]\n",
    "# y[-6685369989889550852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx[2][1].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(noun)):\n",
    "    for j in range(len(noun[i])):\n",
    "        if ('Malware' == noun[i][j]):\n",
    "            count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\n",
      "Noun phrases: ['CVE-2019-897 NSA', \"EternalRomance 'formed basis\", \"alleged Chinese tool' - iTWire Researchers Mark Lechtik\", 'Nadav Grossman', 'the hacking group', 'APT3', 'Gothic Panda', 'its exploitation tool', 'https://ift.tt/2HRG9js']\n",
      "Verbs: ['exploit', 'form', 'say', 'offer', 'exploit', 'hack', 'know', 'base']\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "# print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  The texas virus incident is getting severe\n",
      "texas GPE\n",
      "Noun phrases: ['The texas virus incident']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "sentence = \"The texas virus incident is getting severe\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "# print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(texas,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['The texas virus incident']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['get']\n"
     ]
    }
   ],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texas GPE\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oauth_consumer_key - ZYOj2R1VYsIUhaRvsrcRuHxk6\n",
    "# oauth_nonce\tkYjzVBB8Y0ZFabxSWbWovY3uYSQ2pTgmZeNu2VS4cg\n",
    "# Consumer Secret = API Secret Key = uTwMxW2CPXfv8wZCcqOSX8Yw46eqJVMNopqh5kh9O0VXiVQNRG\n",
    "# Access token secret = OAuth token secret: VB6fxb1akdtwRDf52sbQlPKE9tpHEPX1ZBSI3Y1t33REk\n",
    "# Signing key = uTwMxW2CPXfv8wZCcqOSX8Yw46eqJVMNopqh5kh9O0VXiVQNRG&VB6fxb1akdtwRDf52sbQlPKE9tpHEPX1ZBSI3Y1t33REk\n",
    "# oauth-token = 797644398670409728-I3aUqpmhD7uPscfFZFxMGfMWXPBmiaN\n",
    "# oauth_signature = Use HMAC\n",
    "# curl -XPOST \n",
    "#   --url 'https://api.twitter.com/1.1/statuses/update.json?status=hello' \n",
    "#   --header 'authorization: OAuth\n",
    "#   oauth_consumer_key=\"oauth_customer_key\",\n",
    "#   oauth_nonce=\"generated_oauth_nonce\",\n",
    "#   oauth_signature=\"generated_oauth_signature\",\n",
    "#   oauth_signature_method=\"HMAC-SHA1\",\n",
    "#   oauth_timestamp=\"generated_timestamp\",\n",
    "#   oauth_token=\"797644398670409728-I3aUqpmhD7uPscfFZFxMGfMWXPBmiaN\",\n",
    "#   oauth_version=\"1.0\"'\n",
    "\n",
    "# NEW KEYS: (DO NOT SHARE)\n",
    "# Access token :797644398670409728-Zwgcl9kcCFerhFNlFFGwR3emSbfpfpX\n",
    "# Copy\n",
    "# Access token secret :CzVCCqD8X9FC059X98deDiNYb24IWjhZYVeAhoU4F5v7l\n",
    "# Copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "api = twitter.Api(consumer_key=\"ZYOj2R1VYsIUhaRvsrcRuHxk6\",\n",
    "                  consumer_secret=\"uTwMxW2CPXfv8wZCcqOSX8Yw46eqJVMNopqh5kh9O0VXiVQNRG\",\n",
    "                  access_token_key=\"797644398670409728-Zwgcl9kcCFerhFNlFFGwR3emSbfpfpX\",\n",
    "                  access_token_secret=\"CzVCCqD8X9FC059X98deDiNYb24IWjhZYVeAhoU4F5v7l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = api.GetSearch(\n",
    "    raw_query=\"q=from%3Atwitterdev&result_type=mixed&count=2&tweet_mode=extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[27].AsJsonString\n",
    "x = results[0].AsDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dir(twitter.models.Status)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Status(ID=1238217454226042880, ScreenName=BloomTV, Created=Thu Mar 12 21:37:00 +0000 2020, Text='Downtown San Francisco is so quiet it feels like a Sunday. At every table I pass, I hear conversation about the‚Ä¶ https://t.co/BBVTjPL1pd'),\n",
       " Status(ID=1238280494011334658, ScreenName=SFPublicLibrary, Created=Fri Mar 13 01:47:30 +0000 2020, Text='We know it‚Äôs a scary time, but don‚Äôt lose your #Census. \\n\\nThe 2020 Census kicked off today, and we‚Äôre joining the e‚Ä¶ https://t.co/jEmOKbCSgp'),\n",
       " Status(ID=1238398634766581761, ScreenName=20thcenturygoth, Created=Fri Mar 13 09:36:57 +0000 2020, Text='The Office\\n\\nWow!\\n\\n#theoffice #dwight #pepperspray #pam #jim #roy @ San Francisco, California, U.S.A https://t.co/6bvNCb3ddI')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> api.GetSearch(geocode=[37.781157, -122.398720, \"1mi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Tue Mar 10 17:47:52 +0000 2020',\n",
       " 'favorite_count': 78,\n",
       " 'full_text': 'Today we‚Äôre introducing a simplified &amp; more straightforward set of rules for developers. This policy is an important part of our commitment to protecting the safety &amp; privacy of the people who use our service. https://t.co/2ZYxJ2jOOO\\n\\nRead on for highlights from the policy \\U0001f9f5',\n",
       " 'hashtags': [],\n",
       " 'id': 1237435016134656006,\n",
       " 'id_str': '1237435016134656006',\n",
       " 'lang': 'en',\n",
       " 'retweet_count': 39,\n",
       " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'urls': [{'expanded_url': 'https://blog.twitter.com/developer/en_us/topics/community/2020/twitter_developer_policy_update.html',\n",
       "   'url': 'https://t.co/2ZYxJ2jOOO'}],\n",
       " 'user': {'created_at': 'Sat Dec 14 04:35:55 +0000 2013',\n",
       "  'description': \"The voice of Twitter's #DevRel team, and your official source for updates, news, & events about Twitter's API.\\n\\nNeed help? Visit https://t.co/DVDf7qKyS9\",\n",
       "  'favourites_count': 2188,\n",
       "  'followers_count': 508102,\n",
       "  'friends_count': 1801,\n",
       "  'geo_enabled': True,\n",
       "  'id': 2244994945,\n",
       "  'id_str': '2244994945',\n",
       "  'listed_count': 1657,\n",
       "  'location': '127.0.0.1',\n",
       "  'name': 'Twitter Dev',\n",
       "  'profile_background_color': 'FFFFFF',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/2244994945/1498675817',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/880136122604507136/xHrnqf1T_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/880136122604507136/xHrnqf1T_normal.jpg',\n",
       "  'profile_link_color': '0084B4',\n",
       "  'profile_sidebar_border_color': 'FFFFFF',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'screen_name': 'TwitterDev',\n",
       "  'statuses_count': 3535,\n",
       "  'url': 'https://t.co/3ZX3TNiZCY',\n",
       "  'verified': True},\n",
       " 'user_mentions': []}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
