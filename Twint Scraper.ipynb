{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "file = open(\"../security_tags.txt\",\"r\")\n",
    "arr = []\n",
    "line1 = file.readlines()\n",
    "for x in line1:\n",
    "    arr.append(x)\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(arr)):\n",
    "    arr[i] = arr[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./test1.json\",\"w\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear file\n",
    "# #Twint Search\n",
    "# for i in range(len(arr)):\n",
    "#     d = twint.Config()\n",
    "#     d.Search = arr[i]\n",
    "#     d.Limit = 50\n",
    "#     # d.Store_csv = True\n",
    "#     d.Store_object = True\n",
    "#     d.Store_json = True\n",
    "#     d.Output = \"./test1.json\"\n",
    "#     # d.Database = \"tweets.db\"\n",
    "\n",
    "\n",
    "#     x = twint.run.Search(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear file\n",
    "# Twint Search\n",
    "d = twint.Config()\n",
    "d.Search = \"virustotal\"\n",
    "# d.Search = \"endpoint AND seminar\"\n",
    "d.Limit = 1000\n",
    "# d.Store_csv = True\n",
    "d.Store_object = True\n",
    "d.Store_json = True\n",
    "d.Output = \"./test1.json\"\n",
    "# d.Database = \"tweets.db\"\n",
    "\n",
    "x = twint.run.Search(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search terms\n",
    "# 0-day AND exploit\n",
    "# “0-day”, “CVE-“, “CVE-2018-*”, “CVE-2020-*”\n",
    "# usernames:\n",
    "# kibbsy\n",
    "# it_securitynews\n",
    "# msftsecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = twint.run.Search(d)\n",
    "# twint -u noneprivacy --csv --output \"/Users/psingh4/harsh/test4.json\" --lang en --translate --translate-dest it --limit 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twint.run.Search(d)\n",
    "# twint.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Loads file written from Twint\n",
    "################################################################################################\n",
    "\n",
    "f= open(\"./test1.json\",\"r\",errors='ignore')\n",
    "line1 = f.readlines()\n",
    "arr = []\n",
    "for x in line1:\n",
    "    arr.append(x)\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Fixes Error from Twint where braces and commas were missing in the json\n",
    "################################################################################################\n",
    "arr.insert(0,\"[\")\n",
    "arr.append(']')\n",
    "for i in range(1,len(arr)-2):\n",
    "    arr[i] = arr[i][:-1] + \",\" + arr[i][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Writes the corrected files to json\n",
    "################################################################################################\n",
    "file = open(\"../test.json\",\"w\")\n",
    "for i in range(len(arr)):\n",
    "    file.write(arr[i])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Loads corrected JSON file\n",
    "################################################################################################\n",
    "\n",
    "f= open(\"../test.json\",\"r\",errors='ignore')\n",
    "line1 = f.readlines()\n",
    "data = []\n",
    "for x in line1:\n",
    "    data.append(x)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Removes Non-ASCII characters\n",
    "################################################################################################\n",
    "for i in range(len(data)):\n",
    "    data[i] = (''.join([i if ord(i) < 128 else ' ' for i in str(data[i])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data.json\",\"w\")\n",
    "for i in range(len(data)):\n",
    "    file.write(data[i])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./data.json\", \"r\",errors='ignore') as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id::1220037979818856448:RT @cglyer: BREAKING - To help organizations identify compromised systems with CVE-2019-19781, @FireEye  &amp; @Citr'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[130]['tweet']\n",
    "# is_english(str(data[130]['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroReturnError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: (54, 'ECONNRESET')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroReturnError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', OSError(\"(54, 'ECONNRESET')\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-129aa410dd7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# actual source language that will be recognized by Google Translator when the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[1;32m     77\u001b[0m                                     token=token)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mraw_tkk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', OSError(\"(54, 'ECONNRESET')\"))"
     ]
    }
   ],
   "source": [
    "translator.detect(\"Hello\").lang == 'en'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3828fc208703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#     print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-3828fc208703>\u001b[0m in \u001b[0;36mis_english\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# actual source language that will be recognized by Google Translator when the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "################################################################################################\n",
    "# Google Trans model for checking if a tweet is english or not, unpoisons the training data\n",
    "################################################################################################\n",
    "\n",
    "# ! pip install googletrans\n",
    "from googletrans import Translator\n",
    "translator = Translator(service_urls=['translate.google.com'])\n",
    "\n",
    "def is_english(sentence):\n",
    "    if translator.detect(sentence).lang == 'en':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "with open(\"../classified_tweets.json\", \"r\",errors='ignore') as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "i=0\n",
    "skip = [130]\n",
    "while i <= len(data):\n",
    "#     print(i)\n",
    "    if (i in skip): continue\n",
    "    if is_english(data[i]['tweet']) == False:\n",
    "        print(data[i]['tweet'])\n",
    "        data.pop(i)\n",
    "    else: i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# MySql connection established\n",
    "################################################################################################\n",
    "\n",
    "import mysql.connector\n",
    "mydb = mysql.connector.connect(user='admin', password='Private2712!',\n",
    "                              host='database-1.cok63qqiofsd.us-east-1.rds.amazonaws.com',\n",
    "                              database='data')\n",
    "mycursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004 record inserted.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################################################\n",
    "# Insert records from Twint into Relation DB, see relation schema code on GitHub for table structure\n",
    "# Requires the JSON file with metadata\n",
    "###############################################################################################################\n",
    "\n",
    "sql = \"\"\"INSERT INTO classified_tweets (relevant, created_at, conversation_id, id, date, time, timezone, user_id, username, name, place, tweet, mentions, urls, replies_count, \n",
    "        retweets_count, likes_count, hashtags, cashtags, link, retweet, video, near, geo, source, user_rt_id, user_rt, retweet_id,\n",
    "        retweet_date, translate, trans_src, trans_dest, photos, reply_to) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, \n",
    "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "\n",
    "val = []\n",
    "for i in range(1, len(data)):\n",
    "    val.append((1, data[i]['created_at'], data[i]['conversation_id'], str(data[i]['id']), data[i]['date'], data[i]['time'], data[i]['timezone'], data[i]['user_id'], data[i]['username'],\n",
    "      data[i]['name'], data[i]['place'], data[i]['tweet'], str(data[i]['mentions']), str(data[i]['urls']), data[i]['replies_count'],\n",
    "      data[i]['retweets_count'], data[i]['likes_count'], str(data[i]['hashtags']), str(data[i]['cashtags']), data[i]['link'], data[i]['retweet'],\n",
    "      data[i]['video'], data[i]['near'], data[i]['geo'], data[i]['source'], data[i]['user_rt_id'], data[i]['user_rt'],\n",
    "      data[i]['retweet_id'], data[i]['retweet_date'], data[i]['translate'], data[i]['trans_src'], data[i]['trans_dest'],str(data[i]['photos']), \n",
    "       str(data[i]['reply_to'])))\n",
    "    \n",
    "mycursor.executemany(sql, val)\n",
    "\n",
    "mydb.commit()\n",
    "\n",
    "print(mycursor.rowcount, \"record inserted.\")\n",
    "\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Cell to see how the data looks at in val (Outdated, more attributes being captured now)\n",
    "###############################################################################################################\n",
    "\n",
    "val = []\n",
    "for i in range(len(data)):\n",
    "    val.append((data[i]['created_at'], data[i]['conversation_id'], data[i]['date'], data[i]['time'], data[i]['timezone'], data[i]['user_id'], data[i]['username'],\n",
    "      data[i]['name'], data[i]['place'], data[i]['tweet'], str(data[i]['mentions']), str(data[i]['urls']), data[i]['replies_count'],\n",
    "      data[i]['retweets_count'], data[i]['likes_count'], str(data[i]['hashtags']), str(data[i]['cashtags']), data[i]['link'], data[i]['retweet'],\n",
    "      data[i]['video'], data[i]['near'], data[i]['geo'], data[i]['source'], data[i]['user_rt_id'], data[i]['user_rt'],\n",
    "      data[i]['retweet_id'], data[i]['retweet_date'], data[i]['translate'], data[i]['trans_src'], data[i]['trans_dest'],0,str(data[i]['photos']), \n",
    "       str(data[i]['reply_to'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydb.commit()\n",
    "# !pip install tensorflow\n",
    "# print(mycursor.rowcount, \"record inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert-serving-server\n",
    "# !pip install bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentence = \"NSA exploit EternalRomance with CVE-2019-897 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "# sentence = \"NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) · CVE-2020-7598 · GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) · CVE-2020-7598 · GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7\n",
      "[NowBrowsing, CVE-2020-7598]\n"
     ]
    }
   ],
   "source": [
    "# sentence = \"NSA exploit EternalRomance with CVE-2019-897 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "sentence = \"NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) · CVE-2020-7598 · GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bert-serving-server bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on termninal if Bert Server and Client installed\n",
    "# !bert-serving-start -model_dir /tmp/english_L-12_H-768_A-12/ -num_worker=4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bert_serving.client import BertClient\n",
    "# bc = BertClient()\n",
    "# bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first neural network with keras tutorial\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# load the dataset\n",
    "data = np.load('./tweet_encodings.npy')\n",
    "label = np.load('./maybeincludedlabels.npy')\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(400, input_dim=768, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "# evaluate the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data, label, epochs=10, batch_size=1, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "len(data), len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=1).fit(data[:-900], label[:-900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(data[-900:], label[-900:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(data[-800:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = label[-800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(y_true,y_pred)\n",
    "print(\"Accuracy score: {}\".format(accuracy_score))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true,y_pred,pos_label = 1)\n",
    "auc_score = metrics.auc(fpr,tpr)\n",
    "print(\"AUC: {}\".format(auc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def print_metrics(y_true,y_pred):\n",
    "    accuracy_score = metrics.accuracy_score(y_true,y_pred)\n",
    "    print(\"Accuracy score: {}\".format(accuracy_score))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true,y_pred,pos_label = 1)\n",
    "    auc_score = metrics.auc(fpr,tpr)\n",
    "    print(\"AUC: {}\".format(auc_score))\n",
    "    return accuracy_score,auc_score\n",
    "\n",
    "def logistic_regression(x_train,x_test,y_train,y_test):\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(x_train,y_train)\n",
    "    predictions = logisticRegr.predict(x_test)\n",
    "    print_metrics(y_test,predictions)\n",
    "\n",
    "\n",
    "def support_vector(x_train,x_test,y_train,y_test):\n",
    "    model = svm.SVC()\n",
    "    model.fit(x_train,y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    print_metrics(y_test,predictions)\n",
    "\n",
    "def torch_label_creator(label):\n",
    "    if label.item() == 1:\n",
    "        return torch.tensor([1,0])\n",
    "    else:\n",
    "        return torch.tensor([0,1])\n",
    "\n",
    "    #plot test accuracies and auc over epochs    \n",
    "    plt.plot(test_accuracies)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Test Accuracy vs Epoch')\n",
    "    plt.show()\n",
    "    plt.plot(test_auc)\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Test AUC vs Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = np.load('tweet_encodings.npy')\n",
    "labels = np.load('maybeincludedlabels.npy')\n",
    "x_train,x_test,y_train,y_test = train_test_split(encodings,labels, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost\",auth=(\"test_user\",\"password\"), encrypted=False)\n",
    "#Parse data\n",
    "counter = 0\n",
    "with open(\"./data.json\",'r') as file:\n",
    "    #Need to make a session where you will run all your cypher queries\n",
    "    with driver.session() as session:\n",
    "        tx = session.begin_transaction()\n",
    "        noun_count = 0\n",
    "        count = 0\n",
    "        for line in file.readlines(): #can limit lines with [:100] after ()\n",
    "            if (line == \"[\\n\" or line == \"]\"):\n",
    "                continue\n",
    "            item = json.loads(line[:-2])\n",
    "            sen_noun = noun[noun_count]\n",
    "            # print(item)\n",
    "            tx.run('''CREATE (a:Tweet{id:$value.id,date:$value.date,train_id:$value.train_id,nouns:$sen_noun})''',\n",
    "                   parameters={'tweet': item}, value=item, sen_noun=sen_noun) # add parameters here\n",
    "            #Batch processing to run 1000 tweets as a time as these commits are quite time intensive\n",
    "            count += 1\n",
    "            noun_count += 1\n",
    "#             tx.commit()\n",
    "#             break   # UNCOMMENT IF PUSHING MULTIPLE ROWS/NODES OF DATA \n",
    "            if count > 1000:\n",
    "                tx.commit()\n",
    "                tx = session.begin_transaction()\n",
    "                count = 0\n",
    "        tx.commit()\n",
    "# import json\n",
    "# with open(\"./data.json\", \"r\") as read_file:\n",
    "#     for line in read_file.readlines():\n",
    "#         print(line)\n",
    "#         if (line == \"[\\n\" or line == \"]\"):\n",
    "#             continue\n",
    "#         item = json.loads(line[:-2])\n",
    "#         print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Cypher code for graph db (Some example queries)\n",
    "########################################################################################################\n",
    "\n",
    "# Match (a:Tweet) where 'Malware' in a.nouns return a\n",
    "\n",
    "# Match (a:Tweet), (b:Tweet) where 'Malware' in a.nouns and 'Malware' in b.nouns create (a)-[r:Malware]->(b) return r\n",
    "\n",
    "# '''WITH {tweet} AS Tweet\n",
    "#     Merge (a:Tweet{id:$value.id,date:value.date,train_id:value.train_id})\n",
    "#     Merge (d:Date{date:value.date})\n",
    "#     Merge (a)-[:SAME_DATE]->(d)'''\n",
    "\n",
    "# '''CREATE (a:Tweet{id:$value.id,date:$value.date,train_id:$value.train_id,nouns=sen_noun})'''\n",
    "\n",
    "# '''start n=node(*) return n'''\n",
    "\n",
    "# MATCH (n:Tweet)-[m:Malware]-(b:Tweet)\n",
    "#     WHERE n.id = 1223562341540859904\n",
    "#     REturn b\n",
    "#     order by b.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Connection to graph db (Neo4j) established\n",
    "########################################################################################################\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\",auth=(\"test_user\",\"password\"), encrypted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {}\n",
    "with open(\"./datasets/complete_data.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\n",
      "Noun phrases: ['CVE-2019-897 NSA', \"EternalRomance 'formed basis\", \"alleged Chinese tool' - iTWire Researchers Mark Lechtik\", 'Nadav Grossman', 'the hacking group', 'APT3', 'Gothic Panda', 'its exploitation tool', 'https://ift.tt/2HRG9js']\n"
     ]
    }
   ],
   "source": [
    "# Note, apply some more feature engineering such as removing stop words from a sentence\n",
    "# techniques laid out here: https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "# print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Uses Spacy for finding nouns, named entity recognition\n",
    "# Old results from spacy, without removing stop words\n",
    "########################################################################################################\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Option = 1 => Get all Nouns, Oprion = 2 => Get position entities\n",
    "def get_all_nouns( data, option ):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    sentence = data['text'].replace(\"-\", \" \").replace(\"#\", \" \")\n",
    "    doc = nlp(sentence)\n",
    "    if option == 1:\n",
    "        tagged = [chunk.text for chunk in doc.noun_chunks]\n",
    "        return tagged\n",
    "    else:\n",
    "        pos_ent = []\n",
    "        for ent in doc.ents:\n",
    "            pos_ent.append(ent.text)\n",
    "#             print(ent.text, ent.label_)\n",
    "        return pos_ent\n",
    "\n",
    "########################################################################################################\n",
    "# Uses Spacy for finding nouns, named entity recognition\n",
    "########################################################################################################\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# noun = []\n",
    "# noun_from_tweet = []\n",
    "# for i in range(len(data)):\n",
    "#     if data[i]['relevant'] == 0:\n",
    "#         continue\n",
    "#     sen_noun = []\n",
    "#     sentence = data[i]['text']\n",
    "#     doc = nlp(sentence)\n",
    "#     tagged = [chunk.text for chunk in doc.noun_chunks]\n",
    "#     noun_from_tweet.append(tagged)\n",
    "#     for nouns in tagged:\n",
    "#         noun.append(nouns)\n",
    "# #     noun.append(sen_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Uses Spacy for finding nouns, named entity recognition\n",
    "# NEW results from spacy, after removing stop words\n",
    "########################################################################################################\n",
    "\n",
    "def get_nouns_wo_stop_words (data, option):\n",
    "    nlp = English()\n",
    "\n",
    "    my_doc = nlp(data['text'].replace(\"-\", \" \").replace(\"#\", \" \"))\n",
    "    # Create list of word tokens\n",
    "    token_list = []\n",
    "    for token in my_doc:\n",
    "        token_list.append(token.text)\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "    # Create list of word tokens after removing stopwords\n",
    "    filtered_sentence =[] \n",
    "\n",
    "    for word in token_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word)    # Contains the sentence without stop words\n",
    "    # print(token_list)\n",
    "    # print(filtered_sentence) \n",
    "\n",
    "    new_text = filtered_sentence[0] + \" \"\n",
    "    for i in range(1,len(filtered_sentence)):\n",
    "        new_text += filtered_sentence[i] + \" \"\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "    noun = []\n",
    "    sen_noun = []\n",
    "    sentence = new_text\n",
    "    doc = nlp(sentence)\n",
    "    if option == 1:\n",
    "        tagged = [chunk.text for chunk in doc.noun_chunks]\n",
    "        return tagged\n",
    "    else:\n",
    "        pos_ent = []\n",
    "        for ent in doc.ents:\n",
    "            pos_ent.append(ent.text)\n",
    "#             print(ent.text, ent.label_)\n",
    "        return pos_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Uses NLTK for finding nouns\n",
    "########################################################################################################\n",
    "\n",
    "# nltk.download()    #Crashes on Mac, check on win machine\n",
    "# Bottom two fixes the issue by not using nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('english')\n",
    "\n",
    "# This cell for nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "def nltk_nouns(data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    sen_noun = []\n",
    "    sentence = data['text']\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    for j in range(len(tagged)):\n",
    "        if (tagged[j][1] == 'NNP'):\n",
    "            sen_noun.append(tagged[j][0])\n",
    "    return sen_noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# Test between with and without stop words noun results for spacy\n",
    "##############################################################################################################\n",
    "\n",
    "# Collecting 100 tweets\n",
    "counter = 0\n",
    "data_list = []\n",
    "for i in range(len(data)):\n",
    "    if data[i]['relevant'] == 1:\n",
    "        data_list.append(data[i])\n",
    "        counter += 1\n",
    "    if (counter == 100): break\n",
    "# data[2788]\n",
    "# get_all_nouns(data[2788])\n",
    "\n",
    "results = []\n",
    "for i in range(100):\n",
    "    results.append((get_all_nouns(data_list[i], 2), get_nouns_wo_stop_words(data_list[i], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], []),\n",
       " (['LokiBot', 'AgentTesla'], ['AgentTesla']),\n",
       " ([], []),\n",
       " (['VB6', '@JAMESWT_MHT @wwp96 @unpacme'],\n",
       "  ['VB6', '@JAMESWT_MHT @wwp96 @unpacme']),\n",
       " (['LokiBot', 'AgentTesla'], ['AgentTesla']),\n",
       " ([\"last week's\",\n",
       "   'Emotet 255',\n",
       "   '270',\n",
       "   '235',\n",
       "   '211',\n",
       "   'AgentTesla',\n",
       "   '167',\n",
       "   '325',\n",
       "   '125'],\n",
       "  ['270', '235', '211', '167', '325']),\n",
       " (['Agenttesla', 'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt'],\n",
       "  ['Agenttesla', 'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt']),\n",
       " (['Agenttesla', 'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt'],\n",
       "  ['Agenttesla', 'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt']),\n",
       " (['Agenttesla',\n",
       "   'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt',\n",
       "   '24',\n",
       "   '05:38:37',\n",
       "   '0800',\n",
       "   'Exfil',\n",
       "   'klai.epicbrokers[.]com'],\n",
       "  ['UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt',\n",
       "   '24',\n",
       "   '05:38:37   0800',\n",
       "   'Exfil',\n",
       "   'klai.epicbrokers[.]com']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['https://t.co/E9lvv408QH'], ['https://t.co/E9lvv408QH']),\n",
       " (['Malware', 'Stealer', 'AnyRun'], ['Stealer']),\n",
       " ([\"last week's\",\n",
       "   'Emotet 255',\n",
       "   '270',\n",
       "   '235',\n",
       "   '211',\n",
       "   'AgentTesla',\n",
       "   '167',\n",
       "   '325',\n",
       "   '125'],\n",
       "  ['270', '235', '211', '167', '325']),\n",
       " (['AggahCampaign',\n",
       "   'RGCampaign',\n",
       "   'Botnet',\n",
       "   'Malware',\n",
       "   'CyberAttack',\n",
       "   'CyberSecurity'],\n",
       "  ['RGCampaign', 'Botnet', 'CyberAttack', 'CyberSecurity', 'Aggah']),\n",
       " (['AggahCampaign',\n",
       "   'RGCampaign',\n",
       "   'Botnet',\n",
       "   'Malware',\n",
       "   'CyberAttack',\n",
       "   'CyberSecurity'],\n",
       "  ['RGCampaign', 'Botnet', 'CyberAttack', 'CyberSecurity', 'Aggah']),\n",
       " (['AggahCampaign',\n",
       "   'RGCampaign',\n",
       "   'Botnet',\n",
       "   'Malware',\n",
       "   'CyberAttack',\n",
       "   'CyberSecurity'],\n",
       "  ['RGCampaign', 'Botnet', 'CyberAttack', 'CyberSecurity', 'Aggah']),\n",
       " (['AggahCampaign',\n",
       "   'RGCampaign',\n",
       "   'Botnet',\n",
       "   'Malware',\n",
       "   'CyberAttack',\n",
       "   'CyberSecurity'],\n",
       "  ['RGCampaign', 'Botnet', 'CyberAttack', 'CyberSecurity']),\n",
       " (['more than a year',\n",
       "   'https://t.co/PiWGKFX0tC',\n",
       "   'Malware',\n",
       "   'AggahCampaign',\n",
       "   'Malware'],\n",
       "  ['https://t.co/PiWGKFX0tC']),\n",
       " (['Botnet'], ['Botnet'])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[30:50]\n",
    "# Note have all occurences of zero day collapse to 0-day node in graph db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'RT @DBMCSR: #Microsoft Releases Advisory on Zero-Day #Vulnerability CVE-2020-0674, Workaround Provided\\nhttps://t.co/61dGXiTHhO\\n#0Day #ZeroD…',\n",
       " 'relevant': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'RT @DBMCSR: #Microsoft Releases Advisory on Zero-Day #Vulnerability CVE-2020-0674, Workaround Provided\\nhttps://t.co/61dGXiTHhO\\n#0Day #ZeroD…',\n",
       " 'relevant': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################################################################################\n",
    "# Pulling out every hashtag and @mentions from a Tweet\n",
    "##############################################################################################################\n",
    "\n",
    "import re\n",
    "\n",
    "# Data is only a string, not dictionary element\n",
    "def ret_hash_at (data):\n",
    "    hashes = re.findall('#\\w+', data)   # #\\w+ is a regular expression for removing words starting with #\n",
    "    ats = re.findall('@\\w+', data)      # @\\w+ is a regular expression for removing words starting with @\n",
    "    # m = re.search('#\\w+', data_list[28]['text'])  # Search used for checking existence, stops after finding first match, will skip any subsequent parts of the string\n",
    "    # m.group(0)\n",
    "    return (hashes, ats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# Print results for comparing w/wo spacy\n",
    "##############################################################################################################\n",
    "\n",
    "# results[28]\n",
    "(get_all_nouns(data_list[28]), get_nouns_wo_stop_words(data_list[28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adwind Massive Ongoing Phishing Campaign https://t.co/XNVzXgWVjP',\n",
       " 'Turkish Rat',\n",
       " 'a Massive Ongoing Phishing Campaign https://t.co/XNVzXgWVjP',\n",
       " '“The Turkish Rat” Evolved Adwind'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = set()\n",
    "for i in range(len(results[28][0])):\n",
    "    x.add(results[28][0][i])\n",
    "for i in range(len(results[28][1])):\n",
    "    x.add(results[28][1][i])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Security researchers spotted the latest iteration of an evolving Turkish phishing campaign that’s targeting more than 80 companies with Adwind malware. https://t.co/k02mPIz17R @IBMSecurity #Adwind #malware https://t.co/P45lVKAcpY', 'relevant': 1}\n",
      "{'text': '“The Turkish Rat” Evolved Adwind in a Massive Ongoing Phishing Campaign https://t.co/XNVzXgWVjP @CheckPointSW #Adwind #malware https://t.co/7XUqabwEYQ', 'relevant': 1}\n",
      "Turkish NORP\n",
      "more than 80 CARDINAL\n",
      "Adwind GPE\n",
      "Adwind GPE\n",
      "['Security researchers', 'the latest iteration', 'an evolving Turkish phishing campaign', 'more than 80 companies', 'Adwind malware', 'Adwind  malware https://t.co/P45lVKAcpY']\n"
     ]
    }
   ],
   "source": [
    "print(data_list[26])\n",
    "print(data_list[28])\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "noun = []\n",
    "sen_noun = []\n",
    "sentence = data_list[26]['text'].replace(\"-\", \" \").replace(\"#\", \" \")\n",
    "doc = nlp(sentence)\n",
    "tagged = [chunk.text for chunk in doc.noun_chunks]\n",
    "# ADD BELOW TEXT TO FUNCTIONS (MAKE FEATURE FOR NOUNS OR NAMED ENTITIES)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_id': 3354,\n",
       " 'relevant': 1,\n",
       " 'created_at': 1579737681000,\n",
       " 'date': '2020-01-22',\n",
       " 'time': '16:01:21',\n",
       " 'timezone': 'Pacific Standard Time',\n",
       " 'user_id': 2147483647,\n",
       " 'username': 'vulmonfeeds',\n",
       " 'name': 'Vulmon Vulnerability Feed',\n",
       " 'place': '',\n",
       " 'tweet': 'CVE-2019-19841\\n\\nemfd in Ruckus Wireless Unleashed through 200.7.10.102.64 allows remote attackers to execute OS commands via a POST request with the attribute xcmd=packet-capture to admin/_cmdstat.jsp via the mac at...\\n\\n http://vulmon.com/vulnerabilitydetails?qid=CVE-2019-19841     ',\n",
       " 'mentions': '[]',\n",
       " 'urls': \"['http://vulmon.com/vulnerabilitydetails?qid=\",\n",
       " 'replies_count': 0,\n",
       " 'retweets_count': 0,\n",
       " 'likes_count': 0,\n",
       " 'hashtags': '[]',\n",
       " 'cashtags': '[]',\n",
       " 'link': 'https://twitter.com/VulmonFeeds/status/122013',\n",
       " 'retweet': 0,\n",
       " 'video': '0',\n",
       " 'near': '',\n",
       " 'geo': '',\n",
       " 'source': '',\n",
       " 'user_rt_id': 0,\n",
       " 'user_rt': '',\n",
       " 'retweet_id': 0,\n",
       " 'reply_to': \"[{'user_id': '941389496771399680', 'username'\",\n",
       " 'retweet_date': '',\n",
       " 'translate': '',\n",
       " 'trans_src': '',\n",
       " 'trans_dest': '',\n",
       " 'photos': '[]',\n",
       " 'conversation_id': '1220134385472675846',\n",
       " 'id': '1220134385472675846'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16899, 60907)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_list = noun\n",
    "len(set(noun)), len(noun)\n",
    "# noun_list = noun_listt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Neo4j test for noun as node (required another db to store tweet)\n",
    "# Note we only create nodes here and not the relationships\n",
    "################################################################################################\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\",auth=(\"test_user\",\"password\"), encrypted=False)\n",
    "\n",
    "# idd = 2\n",
    "with driver.session() as session:\n",
    "    tx = session.begin_transaction()\n",
    "    count = 0\n",
    "    idd = 0\n",
    "    for noun in noun_list[30000:40000]:\n",
    "#         tx.run('''Match (a:Tweet {type: $word}) return a.num''', word=noun)\n",
    "        result = tx.run('''Match (a:Tweet {type: $word}) return a.num''', word=noun)\n",
    "        x = result.records()\n",
    "        flag = True\n",
    "        for i in x:\n",
    "            flag = False\n",
    "            tx.run('''MATCH (a:Tweet {type:$word}) SET a.num = a.num+1''', word=noun)\n",
    "        if flag:\n",
    "            tx.run('''CREATE (a:Tweet {type:$word, num:1, id:$idd})''', word=noun, idd=idd)\n",
    "            idd += 1\n",
    "        #Batch processing to run 1000 tweets as a time as these commits are quite time intensive\n",
    "    count += 1\n",
    "    if count > 1000:\n",
    "        tx.commit()\n",
    "        tx = session.begin_transaction()\n",
    "        count = 0\n",
    "    tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-54-1500a3907308>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-1500a3907308>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    for i in range(len())\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\",auth=(\"test_user\",\"password\"), encrypted=False)\n",
    "\n",
    "# driver = GraphDatabase.driver(\"bolt://localhost\",auth=(\"test_user\",\"password\"))\n",
    "x = None\n",
    "with driver.session() as session:\n",
    "    tx = session.begin_transaction()\n",
    "    for i in range(len())\n",
    "    result = tx.run('''Match (a:Tweet {type: word}) return a.num''', word=noun)\n",
    "    x = result.records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "# Neo4j code for forming relations (required another db to store tweet)\n",
    "# Requires data to be in a 2d array. Inner array is a list of nouns/named entities in the same tweet\n",
    "#####################################################################################################\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\",auth=(\"test_user\",\"password\"), encrypted=False)\n",
    "\n",
    "with driver.session() as session:\n",
    "    tx = session.begin_transaction()\n",
    "    count = 0\n",
    "    for nouns in noun_from_tweet[:1000]:\n",
    "        for _noun in nouns:\n",
    "            tx.run('''Match (a:Tweet), (b:Tweet) where $noun = a.type and $noun = b.type\n",
    "            merge (a)-[r: encountred_with {type: $noun}]->(b)''',\n",
    "                   noun=_noun.upper()) # add parameters here\n",
    "        #Batch processing to run 1000 tweets as a time as these commits are quite time intensive\n",
    "        count += 1\n",
    "        if count > 1000:\n",
    "            tx.commit()\n",
    "            tx = session.begin_transaction()\n",
    "            count = 0\n",
    "    tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "# print(result.single()[0])\n",
    "# result.single()[0]\n",
    "type(result)\n",
    "# result.single()[0] == None\n",
    "# result.single()\n",
    "print(type(x))\n",
    "for i in x:\n",
    "    print('inside')\n",
    "    print(i.value() == None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "for i in x:\n",
    "    xx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-14502a1a8b70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# y = hash(xx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# xx[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# y[-6685369989889550852]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# y = hash(xx)\n",
    "xx[2].value(key=1)\n",
    "# xx[2]\n",
    "# y[-6685369989889550852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx[2][1].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(noun)):\n",
    "    for j in range(len(noun[i])):\n",
    "        if ('Malware' == noun[i][j]):\n",
    "            count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\n",
      "Noun phrases: ['CVE-2019-897 NSA', \"EternalRomance 'formed basis\", \"alleged Chinese tool' - iTWire Researchers Mark Lechtik\", 'Nadav Grossman', 'the hacking group', 'APT3', 'Gothic Panda', 'its exploitation tool', 'https://ift.tt/2HRG9js']\n",
      "Verbs: ['exploit', 'form', 'say', 'offer', 'exploit', 'hack', 'know', 'base']\n",
      "NSA ORG\n",
      "EternalRomance ORG\n",
      "Chinese NORP\n",
      "Mark Lechtik PERSON\n",
      "Nadav Grossman PERSON\n",
      "first ORDINAL\n",
      "NSA ORG\n",
      "Gothic Panda PERSON\n",
      "['CVE-2019-897 NSA', \"EternalRomance 'formed basis\", \"alleged Chinese tool' - iTWire Researchers Mark Lechtik\", 'Nadav Grossman', 'the hacking group', 'APT3', 'Gothic Panda', 'its exploitation tool', 'https://ift.tt/2HRG9js']\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "# print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "print([chunk.text for chunk in doc.noun_chunks])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'as',\n",
       " 'aught',\n",
       " 'both',\n",
       " 'each',\n",
       " 'each other',\n",
       " 'either',\n",
       " 'enough',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'few',\n",
       " 'he',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'I',\n",
       " 'idem',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'many',\n",
       " 'me',\n",
       " 'mine',\n",
       " 'most',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'naught',\n",
       " 'neither',\n",
       " 'no one',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'nothing',\n",
       " 'nought',\n",
       " 'one',\n",
       " 'one another',\n",
       " 'other',\n",
       " 'others',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourself',\n",
       " 'ourselves',\n",
       " 'several',\n",
       " 'she',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'somewhat',\n",
       " 'such',\n",
       " 'suchlike',\n",
       " 'that',\n",
       " 'thee',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'theirself',\n",
       " 'theirselves',\n",
       " 'them',\n",
       " 'themself',\n",
       " 'themselves',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thine',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thou',\n",
       " 'thy',\n",
       " 'thyself',\n",
       " 'us',\n",
       " 'we',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'whatnot',\n",
       " 'whatsoever',\n",
       " 'whence',\n",
       " 'where',\n",
       " 'whereby',\n",
       " 'wherefrom',\n",
       " 'wherein',\n",
       " 'whereinto',\n",
       " 'whereof',\n",
       " 'whereon',\n",
       " 'wherever',\n",
       " 'wheresoever',\n",
       " 'whereto',\n",
       " 'whereunto',\n",
       " 'wherewith',\n",
       " 'wherewithal',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'whichsoever',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whom',\n",
       " 'whomever',\n",
       " 'whomso',\n",
       " 'whomsoever',\n",
       " 'whose',\n",
       " 'whosever',\n",
       " 'whosesoever',\n",
       " 'whoso',\n",
       " 'whosoever',\n",
       " 'ye',\n",
       " 'yon',\n",
       " 'yonder',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouns = \"\"\"all\n",
    "another\n",
    "any\n",
    "anybody\n",
    "anyone\n",
    "anything\n",
    "as\n",
    "aught\n",
    "both\n",
    "each\n",
    "each other\n",
    "either\n",
    "enough\n",
    "everybody\n",
    "everyone\n",
    "everything\n",
    "few\n",
    "he\n",
    "her\n",
    "hers\n",
    "herself\n",
    "him\n",
    "himself\n",
    "his\n",
    "I\n",
    "idem\n",
    "it\n",
    "its\n",
    "itself\n",
    "many\n",
    "me\n",
    "mine\n",
    "most\n",
    "my\n",
    "myself\n",
    "naught\n",
    "neither\n",
    "no one\n",
    "nobody\n",
    "none\n",
    "nothing\n",
    "nought\n",
    "one\n",
    "one another\n",
    "other\n",
    "others\n",
    "ought\n",
    "our\n",
    "ours\n",
    "ourself\n",
    "ourselves\n",
    "several\n",
    "she\n",
    "some\n",
    "somebody\n",
    "someone\n",
    "something\n",
    "somewhat\n",
    "such\n",
    "suchlike\n",
    "that\n",
    "thee\n",
    "their\n",
    "theirs\n",
    "theirself\n",
    "theirselves\n",
    "them\n",
    "themself\n",
    "themselves\n",
    "there\n",
    "these\n",
    "they\n",
    "thine\n",
    "this\n",
    "those\n",
    "thou\n",
    "thy\n",
    "thyself\n",
    "us\n",
    "we\n",
    "what\n",
    "whatever\n",
    "whatnot\n",
    "whatsoever\n",
    "whence\n",
    "where\n",
    "whereby\n",
    "wherefrom\n",
    "wherein\n",
    "whereinto\n",
    "whereof\n",
    "whereon\n",
    "wherever\n",
    "wheresoever\n",
    "whereto\n",
    "whereunto\n",
    "wherewith\n",
    "wherewithal\n",
    "whether\n",
    "which\n",
    "whichever\n",
    "whichsoever\n",
    "who\n",
    "whoever\n",
    "whom\n",
    "whomever\n",
    "whomso\n",
    "whomsoever\n",
    "whose\n",
    "whosever\n",
    "whosesoever\n",
    "whoso\n",
    "whosoever\n",
    "ye\n",
    "yon\n",
    "yonder\n",
    "you\n",
    "your\n",
    "yours\n",
    "yourself\n",
    "yourselves\"\"\"\n",
    "\n",
    "extra_words = ['the']\n",
    "pronouns = pronouns.split(\"\\n\")\n",
    "pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  The texas virus incident is getting severe\n",
      "texas GPE\n",
      "Noun phrases: ['The texas virus incident']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "sentence = \"The texas virus incident is getting severe\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "# print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(texas,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['The texas virus incident']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['get']\n"
     ]
    }
   ],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texas GPE\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oauth_consumer_key - ZYOj2R1VYsIUhaRvsrcRuHxk6\n",
    "# oauth_nonce\tkYjzVBB8Y0ZFabxSWbWovY3uYSQ2pTgmZeNu2VS4cg\n",
    "# Consumer Secret = API Secret Key = uTwMxW2CPXfv8wZCcqOSX8Yw46eqJVMNopqh5kh9O0VXiVQNRG\n",
    "# Access token secret = OAuth token secret: VB6fxb1akdtwRDf52sbQlPKE9tpHEPX1ZBSI3Y1t33REk\n",
    "# Signing key = uTwMxW2CPXfv8wZCcqOSX8Yw46eqJVMNopqh5kh9O0VXiVQNRG&VB6fxb1akdtwRDf52sbQlPKE9tpHEPX1ZBSI3Y1t33REk\n",
    "# oauth-token = 797644398670409728-I3aUqpmhD7uPscfFZFxMGfMWXPBmiaN\n",
    "# oauth_signature = Use HMAC\n",
    "# curl -XPOST \n",
    "#   --url 'https://api.twitter.com/1.1/statuses/update.json?status=hello' \n",
    "#   --header 'authorization: OAuth\n",
    "#   oauth_consumer_key=\"oauth_customer_key\",\n",
    "#   oauth_nonce=\"generated_oauth_nonce\",\n",
    "#   oauth_signature=\"generated_oauth_signature\",\n",
    "#   oauth_signature_method=\"HMAC-SHA1\",\n",
    "#   oauth_timestamp=\"generated_timestamp\",\n",
    "#   oauth_token=\"797644398670409728-I3aUqpmhD7uPscfFZFxMGfMWXPBmiaN\",\n",
    "#   oauth_version=\"1.0\"'\n",
    "\n",
    "# NEW KEYS: (DO NOT SHARE)\n",
    "# Access token :797644398670409728-Zwgcl9kcCFerhFNlFFGwR3emSbfpfpX\n",
    "# Copy\n",
    "# Access token secret :CzVCCqD8X9FC059X98deDiNYb24IWjhZYVeAhoU4F5v7l\n",
    "# Copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "# Twitter API Connection established\n",
    "#####################################################################################################\n",
    "\n",
    "import twitter\n",
    "api = twitter.Api(consumer_key=\"ZYOj2R1VYsIUhaRvsrcRuHxk6\",\n",
    "                  consumer_secret=\"uTwMxW2CPXfv8wZCcqOSX8Yw46eqJVMNopqh5kh9O0VXiVQNRG\",\n",
    "                  access_token_key=\"797644398670409728-Zwgcl9kcCFerhFNlFFGwR3emSbfpfpX\",\n",
    "                  access_token_secret=\"CzVCCqD8X9FC059X98deDiNYb24IWjhZYVeAhoU4F5v7l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = api.GetSearch(\n",
    "    raw_query=\"q=from%3Atwitterdev&result_type=mixed&count=2&tweet_mode=extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[27].AsJsonString\n",
    "x = results[0].AsDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dir(twitter.models.Status)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Status(ID=1238217454226042880, ScreenName=BloomTV, Created=Thu Mar 12 21:37:00 +0000 2020, Text='Downtown San Francisco is so quiet it feels like a Sunday. At every table I pass, I hear conversation about the… https://t.co/BBVTjPL1pd'),\n",
       " Status(ID=1238280494011334658, ScreenName=SFPublicLibrary, Created=Fri Mar 13 01:47:30 +0000 2020, Text='We know it’s a scary time, but don’t lose your #Census. \\n\\nThe 2020 Census kicked off today, and we’re joining the e… https://t.co/jEmOKbCSgp'),\n",
       " Status(ID=1238398634766581761, ScreenName=20thcenturygoth, Created=Fri Mar 13 09:36:57 +0000 2020, Text='The Office\\n\\nWow!\\n\\n#theoffice #dwight #pepperspray #pam #jim #roy @ San Francisco, California, U.S.A https://t.co/6bvNCb3ddI')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> api.GetSearch(geocode=[37.781157, -122.398720, \"1mi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Tue Mar 10 17:47:52 +0000 2020',\n",
       " 'favorite_count': 78,\n",
       " 'full_text': 'Today we’re introducing a simplified &amp; more straightforward set of rules for developers. This policy is an important part of our commitment to protecting the safety &amp; privacy of the people who use our service. https://t.co/2ZYxJ2jOOO\\n\\nRead on for highlights from the policy \\U0001f9f5',\n",
       " 'hashtags': [],\n",
       " 'id': 1237435016134656006,\n",
       " 'id_str': '1237435016134656006',\n",
       " 'lang': 'en',\n",
       " 'retweet_count': 39,\n",
       " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'urls': [{'expanded_url': 'https://blog.twitter.com/developer/en_us/topics/community/2020/twitter_developer_policy_update.html',\n",
       "   'url': 'https://t.co/2ZYxJ2jOOO'}],\n",
       " 'user': {'created_at': 'Sat Dec 14 04:35:55 +0000 2013',\n",
       "  'description': \"The voice of Twitter's #DevRel team, and your official source for updates, news, & events about Twitter's API.\\n\\nNeed help? Visit https://t.co/DVDf7qKyS9\",\n",
       "  'favourites_count': 2188,\n",
       "  'followers_count': 508102,\n",
       "  'friends_count': 1801,\n",
       "  'geo_enabled': True,\n",
       "  'id': 2244994945,\n",
       "  'id_str': '2244994945',\n",
       "  'listed_count': 1657,\n",
       "  'location': '127.0.0.1',\n",
       "  'name': 'Twitter Dev',\n",
       "  'profile_background_color': 'FFFFFF',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/2244994945/1498675817',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/880136122604507136/xHrnqf1T_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/880136122604507136/xHrnqf1T_normal.jpg',\n",
       "  'profile_link_color': '0084B4',\n",
       "  'profile_sidebar_border_color': 'FFFFFF',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'screen_name': 'TwitterDev',\n",
       "  'statuses_count': 3535,\n",
       "  'url': 'https://t.co/3ZX3TNiZCY',\n",
       "  'verified': True},\n",
       " 'user_mentions': []}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
