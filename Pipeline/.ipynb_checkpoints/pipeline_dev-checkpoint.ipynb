{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "file = open(\"../security_tags.txt\",\"r\")\n",
    "arr = []\n",
    "line1 = file.readlines()\n",
    "for x in line1:\n",
    "    arr.append(x)\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(arr)):\n",
    "    arr[i] = arr[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and Clear file test1.json\n",
    "file = open(\"./test1.json\",\"w\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear file\n",
    "# #Twint Search\n",
    "# for i in range(len(arr)):\n",
    "#     d = twint.Config()\n",
    "#     d.Search = arr[i]\n",
    "#     d.Limit = 50\n",
    "#     # d.Store_csv = True\n",
    "#     d.Store_object = True\n",
    "#     d.Store_json = True\n",
    "#     d.Output = \"./test1.json\"\n",
    "#     # d.Database = \"tweets.db\"\n",
    "\n",
    "\n",
    "#     x = twint.run.Search(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twint Search\n",
    "d = twint.Config()\n",
    "d.Search = \"virustotal\"\n",
    "# d.Search = \"endpoint AND seminar\"\n",
    "d.Limit = 1000\n",
    "# d.Store_csv = True\n",
    "d.Store_object = True\n",
    "d.Store_json = True\n",
    "d.Output = \"./test1.json\"\n",
    "# d.Database = \"tweets.db\"\n",
    "\n",
    "x = twint.run.Search(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search terms\n",
    "# 0-day AND exploit\n",
    "# “0-day”, “CVE-“, “CVE-2018-*”, “CVE-2020-*”\n",
    "# usernames:\n",
    "# kibbsy\n",
    "# it_securitynews\n",
    "# msftsecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = twint.run.Search(d)\n",
    "# twint -u noneprivacy --csv --output \"/Users/psingh4/harsh/test4.json\" --lang en --translate --translate-dest it --limit 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twint.run.Search(d)\n",
    "# twint.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Loads file written from Twint\n",
    "################################################################################################\n",
    "\n",
    "f= open(\"./test1.json\",\"r\",errors='ignore')\n",
    "line1 = f.readlines()\n",
    "arr = []\n",
    "for x in line1:\n",
    "    arr.append(x)\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Fixes Error from Twint where braces and commas were missing in the json\n",
    "################################################################################################\n",
    "arr.insert(0,\"[\")\n",
    "arr.append(']')\n",
    "for i in range(1,len(arr)-2):\n",
    "    arr[i] = arr[i][:-1] + \",\" + arr[i][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Writes the corrected files to json\n",
    "################################################################################################\n",
    "file = open(\"../test.json\",\"w\")\n",
    "for i in range(len(arr)):\n",
    "    file.write(arr[i])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Loads corrected JSON file\n",
    "################################################################################################\n",
    "\n",
    "f= open(\"../test.json\",\"r\",errors='ignore')\n",
    "line1 = f.readlines()\n",
    "data = []\n",
    "for x in line1:\n",
    "    data.append(x)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Removes Non-ASCII characters\n",
    "################################################################################################\n",
    "for i in range(len(data)):\n",
    "    data[i] = (''.join([i if ord(i) < 128 else ' ' for i in str(data[i])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data.json\",\"w\")\n",
    "for i in range(len(data)):\n",
    "    file.write(data[i])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./data.json\", \"r\",errors='ignore') as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id::1220037979818856448:RT @cglyer: BREAKING - To help organizations identify compromised systems with CVE-2019-19781, @FireEye  &amp; @Citr'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[130]['tweet']\n",
    "# is_english(str(data[130]['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.detect(\"Hello\").lang == 'en'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../classified_tweets.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3828fc208703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../classified_tweets.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../classified_tweets.json'"
     ]
    }
   ],
   "source": [
    "################################################################################################\n",
    "# Google Trans model for checking if a tweet is english or not, unpoisons the training data\n",
    "################################################################################################\n",
    "\n",
    "# ! pip install googletrans\n",
    "from googletrans import Translator\n",
    "translator = Translator(service_urls=['translate.google.com'])\n",
    "\n",
    "def is_english(sentence):\n",
    "    if translator.detect(sentence).lang == 'en':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "with open(\"../classified_tweets.json\", \"r\",errors='ignore') as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "i=0\n",
    "skip = [130]\n",
    "while i <= len(data):\n",
    "#     print(i)\n",
    "    if (i in skip): continue\n",
    "    if is_english(data[i]['tweet']) == False:\n",
    "        print(data[i]['tweet'])\n",
    "        data.pop(i)\n",
    "    else: i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# MySql connection established\n",
    "################################################################################################\n",
    "\n",
    "import mysql.connector\n",
    "mydb = mysql.connector.connect(user='admin', password='Private2712!',\n",
    "                              host='database-1.cok63qqiofsd.us-east-1.rds.amazonaws.com',\n",
    "                              database='data')\n",
    "mycursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004 record inserted.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################################################\n",
    "# Insert records from Twint into Relation DB, see relation schema code on GitHub for table structure\n",
    "# Requires the JSON file with metadata\n",
    "###############################################################################################################\n",
    "\n",
    "sql = \"\"\"INSERT INTO classified_tweets (relevant, created_at, conversation_id, id, date, time, timezone, user_id, username, name, place, tweet, mentions, urls, replies_count, \n",
    "        retweets_count, likes_count, hashtags, cashtags, link, retweet, video, near, geo, source, user_rt_id, user_rt, retweet_id,\n",
    "        retweet_date, translate, trans_src, trans_dest, photos, reply_to) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, \n",
    "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "\n",
    "val = []\n",
    "for i in range(1, len(data)):\n",
    "    val.append((1, data[i]['created_at'], data[i]['conversation_id'], str(data[i]['id']), data[i]['date'], data[i]['time'], data[i]['timezone'], data[i]['user_id'], data[i]['username'],\n",
    "      data[i]['name'], data[i]['place'], data[i]['tweet'], str(data[i]['mentions']), str(data[i]['urls']), data[i]['replies_count'],\n",
    "      data[i]['retweets_count'], data[i]['likes_count'], str(data[i]['hashtags']), str(data[i]['cashtags']), data[i]['link'], data[i]['retweet'],\n",
    "      data[i]['video'], data[i]['near'], data[i]['geo'], data[i]['source'], data[i]['user_rt_id'], data[i]['user_rt'],\n",
    "      data[i]['retweet_id'], data[i]['retweet_date'], data[i]['translate'], data[i]['trans_src'], data[i]['trans_dest'],str(data[i]['photos']), \n",
    "       str(data[i]['reply_to'])))\n",
    "    \n",
    "mycursor.executemany(sql, val)\n",
    "\n",
    "mydb.commit()\n",
    "\n",
    "print(mycursor.rowcount, \"record inserted.\")\n",
    "\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Cell to see how the data looks at in val (Outdated, more attributes being captured now)\n",
    "###############################################################################################################\n",
    "\n",
    "val = []\n",
    "for i in range(len(data)):\n",
    "    val.append((data[i]['created_at'], data[i]['conversation_id'], data[i]['date'], data[i]['time'], data[i]['timezone'], data[i]['user_id'], data[i]['username'],\n",
    "      data[i]['name'], data[i]['place'], data[i]['tweet'], str(data[i]['mentions']), str(data[i]['urls']), data[i]['replies_count'],\n",
    "      data[i]['retweets_count'], data[i]['likes_count'], str(data[i]['hashtags']), str(data[i]['cashtags']), data[i]['link'], data[i]['retweet'],\n",
    "      data[i]['video'], data[i]['near'], data[i]['geo'], data[i]['source'], data[i]['user_rt_id'], data[i]['user_rt'],\n",
    "      data[i]['retweet_id'], data[i]['retweet_date'], data[i]['translate'], data[i]['trans_src'], data[i]['trans_dest'],0,str(data[i]['photos']), \n",
    "       str(data[i]['reply_to'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydb.commit()\n",
    "# !pip install tensorflow\n",
    "# print(mycursor.rowcount, \"record inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert-serving-server\n",
    "# !pip install bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NSA', 'NNP'),\n",
       " ('exploit', 'VBZ'),\n",
       " ('EternalRomance', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('CVE-2019-897', 'NNP'),\n",
       " (\"'formed\", 'POS'),\n",
       " ('basis', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('alleged', 'VBN'),\n",
       " ('Chinese', 'JJ'),\n",
       " ('tool', 'NN'),\n",
       " (\"'\", 'POS'),\n",
       " ('-', ':'),\n",
       " ('iTWire', 'NN'),\n",
       " ('Researchers', 'NNP'),\n",
       " ('Mark', 'NNP'),\n",
       " ('Lechtik', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Nadav', 'NNP'),\n",
       " ('Grossman', 'NNP'),\n",
       " ('said', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('first', 'JJ'),\n",
       " ('offered', 'VBN'),\n",
       " ('NSA', 'NNP'),\n",
       " ('exploits', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('hacking', 'VBG'),\n",
       " ('group', 'NN'),\n",
       " (',', ','),\n",
       " ('known', 'VBN'),\n",
       " ('variously', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('APT3', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Gothic', 'NNP'),\n",
       " ('Panda', 'NNP'),\n",
       " (',', ','),\n",
       " ('had', 'VBD'),\n",
       " ('based', 'VBN'),\n",
       " ('its', 'PRP$'),\n",
       " ('exploitation', 'NN'),\n",
       " ('tool', 'NN'),\n",
       " (',', ','),\n",
       " ('https', 'NN'),\n",
       " (':', ':'),\n",
       " ('//ift.tt/2HRG9js', 'NN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################################################\n",
    "# Using nltk to display tagging of words\n",
    "###############################################################################################################\n",
    "\n",
    "import nltk\n",
    "sentence = \"NSA exploit EternalRomance with CVE-2019-897 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "# sentence = \"NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) · CVE-2020-7598 · GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Using Spacy to display tagging of words\n",
    "###############################################################################################################\n",
    "\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')  # Loads the english model for spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) · CVE-2020-7598 · GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7\n",
      "[NowBrowsing, CVE-2020-7598]\n"
     ]
    }
   ],
   "source": [
    "# sentence = \"NSA exploit EternalRomance with CVE-2019-897 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "sentence = \"NowBrowsing: ESLint dependencies are vulnerable (ReDoS and Prototype Pollution) · CVE-2020-7598 · GitHub Advisory Database:  https://github.com/advisories/GHSA-7fhm-mqm4-2wp7\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bert-serving-server bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on termninal if Bert Server and Client installed\n",
    "# !bert-serving-start -model_dir /tmp/english_L-12_H-768_A-12/ -num_worker=4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bert_serving.client import BertClient\n",
    "# bc = BertClient()\n",
    "# bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first neural network with keras tutorial\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# load the dataset\n",
    "data = np.load('./tweet_encodings.npy')\n",
    "label = np.load('./maybeincludedlabels.npy')\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(400, input_dim=768, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "# evaluate the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data, label, epochs=10, batch_size=1, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "len(data), len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=1).fit(data[:-900], label[:-900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(data[-900:], label[-900:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(data[-800:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = label[-800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(y_true,y_pred)\n",
    "print(\"Accuracy score: {}\".format(accuracy_score))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true,y_pred,pos_label = 1)\n",
    "auc_score = metrics.auc(fpr,tpr)\n",
    "print(\"AUC: {}\".format(auc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def print_metrics(y_true,y_pred):\n",
    "    accuracy_score = metrics.accuracy_score(y_true,y_pred)\n",
    "    print(\"Accuracy score: {}\".format(accuracy_score))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true,y_pred,pos_label = 1)\n",
    "    auc_score = metrics.auc(fpr,tpr)\n",
    "    print(\"AUC: {}\".format(auc_score))\n",
    "    return accuracy_score,auc_score\n",
    "\n",
    "def logistic_regression(x_train,x_test,y_train,y_test):\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(x_train,y_train)\n",
    "    predictions = logisticRegr.predict(x_test)\n",
    "    print_metrics(y_test,predictions)\n",
    "\n",
    "\n",
    "def support_vector(x_train,x_test,y_train,y_test):\n",
    "    model = svm.SVC()\n",
    "    model.fit(x_train,y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    print_metrics(y_test,predictions)\n",
    "\n",
    "def torch_label_creator(label):\n",
    "    if label.item() == 1:\n",
    "        return torch.tensor([1,0])\n",
    "    else:\n",
    "        return torch.tensor([0,1])\n",
    "\n",
    "    #plot test accuracies and auc over epochs    \n",
    "    plt.plot(test_accuracies)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Test Accuracy vs Epoch')\n",
    "    plt.show()\n",
    "    plt.plot(test_auc)\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Test AUC vs Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = np.load('tweet_encodings.npy')\n",
    "labels = np.load('maybeincludedlabels.npy')\n",
    "x_train,x_test,y_train,y_test = train_test_split(encodings,labels, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost\",auth=(\"test_user\",\"password\"), encrypted=False)\n",
    "#Parse data\n",
    "counter = 0\n",
    "with open(\"./data.json\",'r') as file:\n",
    "    #Need to make a session where you will run all your cypher queries\n",
    "    with driver.session() as session:\n",
    "        tx = session.begin_transaction()\n",
    "        noun_count = 0\n",
    "        count = 0\n",
    "        for line in file.readlines(): #can limit lines with [:100] after ()\n",
    "            if (line == \"[\\n\" or line == \"]\"):\n",
    "                continue\n",
    "            item = json.loads(line[:-2])\n",
    "            sen_noun = noun[noun_count]\n",
    "            # Stores the Tweets nouns in the graph db (Outdated). Here a node is a collection of nouns without a distinct identity\n",
    "            tx.run('''CREATE (a:Tweet{id:$value.id,date:$value.date,train_id:$value.train_id,nouns:$sen_noun})''',\n",
    "                   parameters={'tweet': item}, value=item, sen_noun=sen_noun) # add parameters here\n",
    "            #Batch processing to run 1000 tweets as a time as these commits are quite time intensive\n",
    "            count += 1\n",
    "            noun_count += 1\n",
    "#             tx.commit()\n",
    "#             break   # UNCOMMENT IF PUSHING MULTIPLE ROWS/NODES OF DATA \n",
    "            if count > 1000:\n",
    "                tx.commit()\n",
    "                tx = session.begin_transaction()\n",
    "                count = 0\n",
    "        tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Cypher code for graph db (Some example queries)\n",
    "########################################################################################################\n",
    "\n",
    "# Match (a:Tweet) where 'Malware' in a.nouns return a\n",
    "\n",
    "# Match (a:Tweet), (b:Tweet) where 'Malware' in a.nouns and 'Malware' in b.nouns create (a)-[r:Malware]->(b) return r\n",
    "\n",
    "# '''WITH {tweet} AS Tweet\n",
    "#     Merge (a:Tweet{id:$value.id,date:value.date,train_id:value.train_id})\n",
    "#     Merge (d:Date{date:value.date})\n",
    "#     Merge (a)-[:SAME_DATE]->(d)'''\n",
    "\n",
    "# '''CREATE (a:Tweet{id:$value.id,date:$value.date,train_id:$value.train_id,nouns=sen_noun})'''\n",
    "\n",
    "# '''start n=node(*) return n'''\n",
    "\n",
    "# MATCH (n:Tweet)-[m:Malware]-(b:Tweet)\n",
    "#     WHERE n.id = 1223562341540859904\n",
    "#     REturn b\n",
    "#     order by b.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Connection to graph db (Neo4j) established\n",
    "########################################################################################################\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\",auth=(\"test_user\",\"password\"), encrypted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {}\n",
    "with open(\"./datasets/complete_data.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\n",
      "Noun phrases: ['CVE-2019-897 NSA', \"EternalRomance 'formed basis\", \"alleged Chinese tool' - iTWire Researchers Mark Lechtik\", 'Nadav Grossman', 'the hacking group', 'APT3', 'Gothic Panda', 'its exploitation tool', 'https://ift.tt/2HRG9js']\n"
     ]
    }
   ],
   "source": [
    "# Note, apply some more feature engineering such as removing stop words from a sentence\n",
    "# techniques laid out here: https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "# print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Uses Spacy for finding nouns, named entity recognition\n",
    "# Old results from spacy, without removing stop words\n",
    "########################################################################################################\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Option = 1 => Get all Nouns, Oprion = 2 => Get position entities\n",
    "def get_all_nouns( data, option ):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    sentence = data['text'].replace(\"-\", \" \").replace(\"#\", \" \")\n",
    "    doc = nlp(sentence)\n",
    "    if option == 1:\n",
    "        tagged = [chunk.text for chunk in doc.noun_chunks]\n",
    "        return tagged\n",
    "    else:\n",
    "        pos_ent = []\n",
    "        for ent in doc.ents:\n",
    "            pos_ent.append(ent.text)\n",
    "#             print(ent.text, ent.label_)\n",
    "        return pos_ent\n",
    "\n",
    "########################################################################################################\n",
    "# Uses Spacy for finding nouns, named entity recognition\n",
    "########################################################################################################\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# noun = []\n",
    "# noun_from_tweet = []\n",
    "# for i in range(len(data)):\n",
    "#     if data[i]['relevant'] == 0:\n",
    "#         continue\n",
    "#     sen_noun = []\n",
    "#     sentence = data[i]['text']\n",
    "#     doc = nlp(sentence)\n",
    "#     tagged = [chunk.text for chunk in doc.noun_chunks]\n",
    "#     noun_from_tweet.append(tagged)\n",
    "#     for nouns in tagged:\n",
    "#         noun.append(nouns)\n",
    "# #     noun.append(sen_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Uses Spacy for finding nouns, named entity recognition\n",
    "# NEW results from spacy, after removing stop words\n",
    "########################################################################################################\n",
    "\n",
    "def get_nouns_wo_stop_words (data, option):\n",
    "    nlp = English()\n",
    "\n",
    "    my_doc = nlp(data['text'].replace(\"-\", \" \").replace(\"#\", \" \"))\n",
    "    # Create list of word tokens\n",
    "    token_list = []\n",
    "    for token in my_doc:\n",
    "        token_list.append(token.text)\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "    # Create list of word tokens after removing stopwords\n",
    "    filtered_sentence =[] \n",
    "\n",
    "    for word in token_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word)    # Contains the sentence without stop words\n",
    "    # print(token_list)\n",
    "    # print(filtered_sentence) \n",
    "\n",
    "    new_text = filtered_sentence[0] + \" \"\n",
    "    for i in range(1,len(filtered_sentence)):\n",
    "        new_text += filtered_sentence[i] + \" \"\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "    noun = []\n",
    "    sen_noun = []\n",
    "    sentence = new_text\n",
    "    doc = nlp(sentence)\n",
    "    if option == 1:\n",
    "        tagged = [chunk.text for chunk in doc.noun_chunks]\n",
    "        return tagged\n",
    "    else:\n",
    "        pos_ent = []\n",
    "        for ent in doc.ents:\n",
    "            pos_ent.append(ent.text)\n",
    "#             print(ent.text, ent.label_)\n",
    "        return pos_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Uses NLTK for finding nouns\n",
    "########################################################################################################\n",
    "\n",
    "# nltk.download()    #Crashes on Mac, check on win machine\n",
    "# Bottom two fixes the issue by not using nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('english')\n",
    "\n",
    "# This cell for nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "def nltk_nouns(data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    sen_noun = []\n",
    "    sentence = data['text']\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    for j in range(len(tagged)):\n",
    "        if (tagged[j][1] == 'NNP'):\n",
    "            sen_noun.append(tagged[j][0])\n",
    "    return sen_noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# Test between with and without stop words noun results for spacy\n",
    "##############################################################################################################\n",
    "\n",
    "# Collecting 100 tweets\n",
    "counter = 0\n",
    "data_list = []\n",
    "for i in range(len(data)):\n",
    "    if data[i]['relevant'] == 1:\n",
    "        data_list.append(data[i])\n",
    "        counter += 1\n",
    "    if (counter == 100): break\n",
    "# data[2788]\n",
    "# get_all_nouns(data[2788])\n",
    "\n",
    "results = []\n",
    "for i in range(100):\n",
    "    results.append((get_all_nouns(data_list[i], 2), get_nouns_wo_stop_words(data_list[i], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# Combining results with and without stop words\n",
    "##############################################################################################################\n",
    "\n",
    "# Results is \n",
    "def combine_stopwords_spacey(results)\n",
    "all_named_entities = []\n",
    "for j in range(len(resluts)):\n",
    "    x = set()\n",
    "    for i in range(len(results[28][0])):\n",
    "        x.add(results[j][0][i])\n",
    "    for i in range(len(results[28][1])):\n",
    "        x.add(results[j][1][i])\n",
    "    all_named_entities.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# Print results for comparing w/wo spacy\n",
    "##############################################################################################################\n",
    "\n",
    "# results[28]\n",
    "(get_all_nouns(data_list[28]), get_nouns_wo_stop_words(data_list[28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], []),\n",
       " (['LokiBot', 'AgentTesla'], ['AgentTesla']),\n",
       " ([], []),\n",
       " (['VB6', '@JAMESWT_MHT @wwp96 @unpacme'],\n",
       "  ['VB6', '@JAMESWT_MHT @wwp96 @unpacme']),\n",
       " (['LokiBot', 'AgentTesla'], ['AgentTesla']),\n",
       " ([\"last week's\",\n",
       "   'Emotet 255',\n",
       "   '270',\n",
       "   '235',\n",
       "   '211',\n",
       "   'AgentTesla',\n",
       "   '167',\n",
       "   '325',\n",
       "   '125'],\n",
       "  ['270', '235', '211', '167', '325']),\n",
       " (['Agenttesla', 'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt'],\n",
       "  ['Agenttesla', 'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt']),\n",
       " (['Agenttesla', 'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt'],\n",
       "  ['Agenttesla', 'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt']),\n",
       " (['Agenttesla',\n",
       "   'UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt',\n",
       "   '24',\n",
       "   '05:38:37',\n",
       "   '0800',\n",
       "   'Exfil',\n",
       "   'klai.epicbrokers[.]com'],\n",
       "  ['UK Atomic Energy&lt;dav.langridge@ryanplastics.co.uk&gt',\n",
       "   '24',\n",
       "   '05:38:37   0800',\n",
       "   'Exfil',\n",
       "   'klai.epicbrokers[.]com']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['https://t.co/E9lvv408QH'], ['https://t.co/E9lvv408QH']),\n",
       " (['Malware', 'Stealer', 'AnyRun'], ['Stealer']),\n",
       " ([\"last week's\",\n",
       "   'Emotet 255',\n",
       "   '270',\n",
       "   '235',\n",
       "   '211',\n",
       "   'AgentTesla',\n",
       "   '167',\n",
       "   '325',\n",
       "   '125'],\n",
       "  ['270', '235', '211', '167', '325']),\n",
       " (['AggahCampaign',\n",
       "   'RGCampaign',\n",
       "   'Botnet',\n",
       "   'Malware',\n",
       "   'CyberAttack',\n",
       "   'CyberSecurity'],\n",
       "  ['RGCampaign', 'Botnet', 'CyberAttack', 'CyberSecurity', 'Aggah']),\n",
       " (['AggahCampaign',\n",
       "   'RGCampaign',\n",
       "   'Botnet',\n",
       "   'Malware',\n",
       "   'CyberAttack',\n",
       "   'CyberSecurity'],\n",
       "  ['RGCampaign', 'Botnet', 'CyberAttack', 'CyberSecurity', 'Aggah']),\n",
       " (['AggahCampaign',\n",
       "   'RGCampaign',\n",
       "   'Botnet',\n",
       "   'Malware',\n",
       "   'CyberAttack',\n",
       "   'CyberSecurity'],\n",
       "  ['RGCampaign', 'Botnet', 'CyberAttack', 'CyberSecurity', 'Aggah']),\n",
       " (['AggahCampaign',\n",
       "   'RGCampaign',\n",
       "   'Botnet',\n",
       "   'Malware',\n",
       "   'CyberAttack',\n",
       "   'CyberSecurity'],\n",
       "  ['RGCampaign', 'Botnet', 'CyberAttack', 'CyberSecurity']),\n",
       " (['more than a year',\n",
       "   'https://t.co/PiWGKFX0tC',\n",
       "   'Malware',\n",
       "   'AggahCampaign',\n",
       "   'Malware'],\n",
       "  ['https://t.co/PiWGKFX0tC']),\n",
       " (['Botnet'], ['Botnet'])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[30:50]\n",
    "# Note have all occurences of zero day collapse to 0-day node in graph db, find other occurences of such words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'RT @DBMCSR: #Microsoft Releases Advisory on Zero-Day #Vulnerability CVE-2020-0674, Workaround Provided\\nhttps://t.co/61dGXiTHhO\\n#0Day #ZeroD…',\n",
       " 'relevant': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# Pulling out every hashtag and @mentions from a Tweet\n",
    "##############################################################################################################\n",
    "\n",
    "import re\n",
    "\n",
    "# Data is only a string, not dictionary element\n",
    "def ret_hash_at (data):\n",
    "    hashes = re.findall('#\\w+', data)   # #\\w+ is a regular expression for removing words starting with #\n",
    "    ats = re.findall('@\\w+', data)      # @\\w+ is a regular expression for removing words starting with @\n",
    "    # m = re.search('#\\w+', data_list[28]['text'])  # Search used for checking existence, stops after finding first match, will skip any subsequent parts of the string\n",
    "    # m.group(0)\n",
    "    return (hashes, ats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16899, 60907)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_list = noun\n",
    "len(set(noun)), len(noun)\n",
    "# noun_list = noun_listt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Neo4j test for noun as node (required another db to store tweet)\n",
    "# Note we only create nodes here and not the relationships\n",
    "################################################################################################\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\",auth=(\"test_user\",\"password\"), encrypted=False)\n",
    "\n",
    "# idd = 2\n",
    "with driver.session() as session:\n",
    "    tx = session.begin_transaction()\n",
    "    count = 0\n",
    "    idd = 0\n",
    "    for noun in noun_list[30000:40000]:\n",
    "#         tx.run('''Match (a:Tweet {type: $word}) return a.num''', word=noun)\n",
    "        result = tx.run('''Match (a:Tweet {type: $word}) return a.num''', word=noun)\n",
    "        x = result.records()\n",
    "        flag = True\n",
    "        for i in x:\n",
    "            flag = False\n",
    "            tx.run('''MATCH (a:Tweet {type:$word}) SET a.num = a.num+1''', word=noun)\n",
    "        if flag:\n",
    "            tx.run('''CREATE (a:Tweet {type:$word, num:1, id:$idd})''', word=noun, idd=idd)\n",
    "            idd += 1\n",
    "        #Batch processing to run 1000 tweets as a time as these commits are quite time intensive\n",
    "    count += 1\n",
    "    if count > 1000:\n",
    "        tx.commit()\n",
    "        tx = session.begin_transaction()\n",
    "        count = 0\n",
    "    tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-54-1500a3907308>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-1500a3907308>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    for i in range(len())\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "################################################################################################\n",
    "# Test for loading results  from db for match on word (Named entity)\n",
    "################################################################################################\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\",auth=(\"test_user\",\"password\"), encrypted=False)\n",
    "\n",
    "# driver = GraphDatabase.driver(\"bolt://localhost\",auth=(\"test_user\",\"password\"))\n",
    "x = None\n",
    "with driver.session() as session:\n",
    "    tx = session.begin_transaction()\n",
    "#     for i in range(len())\n",
    "    result = tx.run('''Match (a:Tweet {type: word}) return a.num''', word=noun)\n",
    "    x = result.records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "# Neo4j code for forming relations (required another db to store tweet)\n",
    "# Requires data to be in a 2d array. Inner array is a list of nouns/named entities in the same tweet\n",
    "#####################################################################################################\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "#Indicate to driver where neo4j is running\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\",auth=(\"test_user\",\"password\"), encrypted=False)\n",
    "\n",
    "with driver.session() as session:\n",
    "    tx = session.begin_transaction()\n",
    "    count = 0\n",
    "    for nouns in noun_from_tweet[:1000]:\n",
    "        for _noun in nouns:\n",
    "            tx.run('''Match (a:Tweet), (b:Tweet) where $noun = a.type and $noun = b.type\n",
    "            merge (a)-[r: encountred_with {type: $noun}]->(b)''',\n",
    "                   noun=_noun.upper()) # add parameters here\n",
    "        #Batch processing to run 1000 tweets as a time as these commits are quite time intensive\n",
    "        count += 1\n",
    "        if count > 1000:\n",
    "            tx.commit()\n",
    "            tx = session.begin_transaction()\n",
    "            count = 0\n",
    "    tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "# print(result.single()[0])\n",
    "# result.single()[0]\n",
    "type(result)\n",
    "# result.single()[0] == None\n",
    "# result.single()\n",
    "print(type(x))\n",
    "for i in x:\n",
    "    print('inside')\n",
    "    print(i.value() == None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "for i in x:\n",
    "    xx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-14502a1a8b70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# y = hash(xx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# xx[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# y[-6685369989889550852]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# y = hash(xx)\n",
    "xx[2].value(key=1)\n",
    "# xx[2]\n",
    "# y[-6685369989889550852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx[2][1].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(noun)):\n",
    "    for j in range(len(noun[i])):\n",
    "        if ('Malware' == noun[i][j]):\n",
    "            count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\n",
      "Noun phrases: ['CVE-2019-897 NSA', \"EternalRomance 'formed basis\", \"alleged Chinese tool' - iTWire Researchers Mark Lechtik\", 'Nadav Grossman', 'the hacking group', 'APT3', 'Gothic Panda', 'its exploitation tool', 'https://ift.tt/2HRG9js']\n",
      "Verbs: ['exploit', 'form', 'say', 'offer', 'exploit', 'hack', 'know', 'base']\n",
      "NSA ORG\n",
      "EternalRomance ORG\n",
      "Chinese NORP\n",
      "Mark Lechtik PERSON\n",
      "Nadav Grossman PERSON\n",
      "first ORDINAL\n",
      "NSA ORG\n",
      "Gothic Panda PERSON\n",
      "['CVE-2019-897 NSA', \"EternalRomance 'formed basis\", \"alleged Chinese tool' - iTWire Researchers Mark Lechtik\", 'Nadav Grossman', 'the hacking group', 'APT3', 'Gothic Panda', 'its exploitation tool', 'https://ift.tt/2HRG9js']\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"CVE-2019-897 NSA exploit EternalRomance 'formed basis for alleged Chinese tool' - iTWire Researchers Mark Lechtik and Nadav Grossman said in first offered NSA exploits that the hacking group, known variously as APT3 and Gothic Panda, had based its exploitation tool, https://ift.tt/2HRG9js\"\n",
    "doc = nlp(sentence)\n",
    "print(\"The sentence is: \", sentence)\n",
    "# print([ent for ent in doc.ents])\n",
    "# for ent in doc.ents:\n",
    "# doc\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "print([chunk.text for chunk in doc.noun_chunks])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'as',\n",
       " 'aught',\n",
       " 'both',\n",
       " 'each',\n",
       " 'each other',\n",
       " 'either',\n",
       " 'enough',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'few',\n",
       " 'he',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'I',\n",
       " 'idem',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'many',\n",
       " 'me',\n",
       " 'mine',\n",
       " 'most',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'naught',\n",
       " 'neither',\n",
       " 'no one',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'nothing',\n",
       " 'nought',\n",
       " 'one',\n",
       " 'one another',\n",
       " 'other',\n",
       " 'others',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourself',\n",
       " 'ourselves',\n",
       " 'several',\n",
       " 'she',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'somewhat',\n",
       " 'such',\n",
       " 'suchlike',\n",
       " 'that',\n",
       " 'thee',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'theirself',\n",
       " 'theirselves',\n",
       " 'them',\n",
       " 'themself',\n",
       " 'themselves',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thine',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thou',\n",
       " 'thy',\n",
       " 'thyself',\n",
       " 'us',\n",
       " 'we',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'whatnot',\n",
       " 'whatsoever',\n",
       " 'whence',\n",
       " 'where',\n",
       " 'whereby',\n",
       " 'wherefrom',\n",
       " 'wherein',\n",
       " 'whereinto',\n",
       " 'whereof',\n",
       " 'whereon',\n",
       " 'wherever',\n",
       " 'wheresoever',\n",
       " 'whereto',\n",
       " 'whereunto',\n",
       " 'wherewith',\n",
       " 'wherewithal',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'whichsoever',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whom',\n",
       " 'whomever',\n",
       " 'whomso',\n",
       " 'whomsoever',\n",
       " 'whose',\n",
       " 'whosever',\n",
       " 'whosesoever',\n",
       " 'whoso',\n",
       " 'whosoever',\n",
       " 'ye',\n",
       " 'yon',\n",
       " 'yonder',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################\n",
    "# List of words to remove from matched named entities\n",
    "################################################################################################\n",
    "\n",
    "pronouns = \"\"\"all\n",
    "another\n",
    "any\n",
    "anybody\n",
    "anyone\n",
    "anything\n",
    "as\n",
    "aught\n",
    "both\n",
    "each\n",
    "each other\n",
    "either\n",
    "enough\n",
    "everybody\n",
    "everyone\n",
    "everything\n",
    "few\n",
    "he\n",
    "her\n",
    "hers\n",
    "herself\n",
    "him\n",
    "himself\n",
    "his\n",
    "I\n",
    "idem\n",
    "it\n",
    "its\n",
    "itself\n",
    "many\n",
    "me\n",
    "mine\n",
    "most\n",
    "my\n",
    "myself\n",
    "naught\n",
    "neither\n",
    "no one\n",
    "nobody\n",
    "none\n",
    "nothing\n",
    "nought\n",
    "one\n",
    "one another\n",
    "other\n",
    "others\n",
    "ought\n",
    "our\n",
    "ours\n",
    "ourself\n",
    "ourselves\n",
    "several\n",
    "she\n",
    "some\n",
    "somebody\n",
    "someone\n",
    "something\n",
    "somewhat\n",
    "such\n",
    "suchlike\n",
    "that\n",
    "thee\n",
    "their\n",
    "theirs\n",
    "theirself\n",
    "theirselves\n",
    "them\n",
    "themself\n",
    "themselves\n",
    "there\n",
    "these\n",
    "they\n",
    "thine\n",
    "this\n",
    "those\n",
    "thou\n",
    "thy\n",
    "thyself\n",
    "us\n",
    "we\n",
    "what\n",
    "whatever\n",
    "whatnot\n",
    "whatsoever\n",
    "whence\n",
    "where\n",
    "whereby\n",
    "wherefrom\n",
    "wherein\n",
    "whereinto\n",
    "whereof\n",
    "whereon\n",
    "wherever\n",
    "wheresoever\n",
    "whereto\n",
    "whereunto\n",
    "wherewith\n",
    "wherewithal\n",
    "whether\n",
    "which\n",
    "whichever\n",
    "whichsoever\n",
    "who\n",
    "whoever\n",
    "whom\n",
    "whomever\n",
    "whomso\n",
    "whomsoever\n",
    "whose\n",
    "whosever\n",
    "whosesoever\n",
    "whoso\n",
    "whosoever\n",
    "ye\n",
    "yon\n",
    "yonder\n",
    "you\n",
    "your\n",
    "yours\n",
    "yourself\n",
    "yourselves\"\"\"\n",
    "\n",
    "extra_words = ['the']\n",
    "pronouns = pronouns.split(\"\\n\")\n",
    "pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['get']\n"
     ]
    }
   ],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texas GPE\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Documentation on establishing a connection using the Twitter API\n",
    "################################################################################################\n",
    "\n",
    "# oauth_consumer_key - ZYOj2R1VYsIUhaRvsrcRuHxk6\n",
    "# oauth_nonce\tkYjzVBB8Y0ZFabxSWbWovY3uYSQ2pTgmZeNu2VS4cg\n",
    "# Consumer Secret = API Secret Key = uTwMxW2CPXfv8wZCcqOSX8Yw46eqJVMNopqh5kh9O0VXiVQNRG\n",
    "# Access token secret = OAuth token secret: VB6fxb1akdtwRDf52sbQlPKE9tpHEPX1ZBSI3Y1t33REk\n",
    "# Signing key = uTwMxW2CPXfv8wZCcqOSX8Yw46eqJVMNopqh5kh9O0VXiVQNRG&VB6fxb1akdtwRDf52sbQlPKE9tpHEPX1ZBSI3Y1t33REk\n",
    "# oauth-token = 797644398670409728-I3aUqpmhD7uPscfFZFxMGfMWXPBmiaN\n",
    "# oauth_signature = Use HMAC\n",
    "# curl -XPOST \n",
    "#   --url 'https://api.twitter.com/1.1/statuses/update.json?status=hello' \n",
    "#   --header 'authorization: OAuth\n",
    "#   oauth_consumer_key=\"oauth_customer_key\",\n",
    "#   oauth_nonce=\"generated_oauth_nonce\",\n",
    "#   oauth_signature=\"generated_oauth_signature\",\n",
    "#   oauth_signature_method=\"HMAC-SHA1\",\n",
    "#   oauth_timestamp=\"generated_timestamp\",\n",
    "#   oauth_token=\"797644398670409728-I3aUqpmhD7uPscfFZFxMGfMWXPBmiaN\",\n",
    "#   oauth_version=\"1.0\"'\n",
    "\n",
    "# NEW KEYS: (DO NOT SHARE)\n",
    "# Access token :797644398670409728-Zwgcl9kcCFerhFNlFFGwR3emSbfpfpX\n",
    "# Copy\n",
    "# Access token secret :CzVCCqD8X9FC059X98deDiNYb24IWjhZYVeAhoU4F5v7l\n",
    "# Copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "# Twitter API Connection established\n",
    "#####################################################################################################\n",
    "\n",
    "import twitter\n",
    "api = twitter.Api(consumer_key=\"ZYOj2R1VYsIUhaRvsrcRuHxk6\",\n",
    "                  consumer_secret=\"uTwMxW2CPXfv8wZCcqOSX8Yw46eqJVMNopqh5kh9O0VXiVQNRG\",\n",
    "                  access_token_key=\"797644398670409728-Zwgcl9kcCFerhFNlFFGwR3emSbfpfpX\",\n",
    "                  access_token_secret=\"CzVCCqD8X9FC059X98deDiNYb24IWjhZYVeAhoU4F5v7l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = api.GetSearch(\n",
    "    raw_query=\"q=from%3Atwitterdev&result_type=mixed&count=2&tweet_mode=extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[27].AsJsonString\n",
    "x = results[0].AsDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dir(twitter.models.Status)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Status(ID=1238217454226042880, ScreenName=BloomTV, Created=Thu Mar 12 21:37:00 +0000 2020, Text='Downtown San Francisco is so quiet it feels like a Sunday. At every table I pass, I hear conversation about the… https://t.co/BBVTjPL1pd'),\n",
       " Status(ID=1238280494011334658, ScreenName=SFPublicLibrary, Created=Fri Mar 13 01:47:30 +0000 2020, Text='We know it’s a scary time, but don’t lose your #Census. \\n\\nThe 2020 Census kicked off today, and we’re joining the e… https://t.co/jEmOKbCSgp'),\n",
       " Status(ID=1238398634766581761, ScreenName=20thcenturygoth, Created=Fri Mar 13 09:36:57 +0000 2020, Text='The Office\\n\\nWow!\\n\\n#theoffice #dwight #pepperspray #pam #jim #roy @ San Francisco, California, U.S.A https://t.co/6bvNCb3ddI')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> api.GetSearch(geocode=[37.781157, -122.398720, \"1mi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Tue Mar 10 17:47:52 +0000 2020',\n",
       " 'favorite_count': 78,\n",
       " 'full_text': 'Today we’re introducing a simplified &amp; more straightforward set of rules for developers. This policy is an important part of our commitment to protecting the safety &amp; privacy of the people who use our service. https://t.co/2ZYxJ2jOOO\\n\\nRead on for highlights from the policy \\U0001f9f5',\n",
       " 'hashtags': [],\n",
       " 'id': 1237435016134656006,\n",
       " 'id_str': '1237435016134656006',\n",
       " 'lang': 'en',\n",
       " 'retweet_count': 39,\n",
       " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'urls': [{'expanded_url': 'https://blog.twitter.com/developer/en_us/topics/community/2020/twitter_developer_policy_update.html',\n",
       "   'url': 'https://t.co/2ZYxJ2jOOO'}],\n",
       " 'user': {'created_at': 'Sat Dec 14 04:35:55 +0000 2013',\n",
       "  'description': \"The voice of Twitter's #DevRel team, and your official source for updates, news, & events about Twitter's API.\\n\\nNeed help? Visit https://t.co/DVDf7qKyS9\",\n",
       "  'favourites_count': 2188,\n",
       "  'followers_count': 508102,\n",
       "  'friends_count': 1801,\n",
       "  'geo_enabled': True,\n",
       "  'id': 2244994945,\n",
       "  'id_str': '2244994945',\n",
       "  'listed_count': 1657,\n",
       "  'location': '127.0.0.1',\n",
       "  'name': 'Twitter Dev',\n",
       "  'profile_background_color': 'FFFFFF',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/2244994945/1498675817',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/880136122604507136/xHrnqf1T_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/880136122604507136/xHrnqf1T_normal.jpg',\n",
       "  'profile_link_color': '0084B4',\n",
       "  'profile_sidebar_border_color': 'FFFFFF',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'screen_name': 'TwitterDev',\n",
       "  'statuses_count': 3535,\n",
       "  'url': 'https://t.co/3ZX3TNiZCY',\n",
       "  'verified': True},\n",
       " 'user_mentions': []}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
